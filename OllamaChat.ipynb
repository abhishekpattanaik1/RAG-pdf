{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "OPEN_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "#MODEL = \"gpt-3.5-turbo\"\n",
    "MODEL = \"llama2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWhy don't scientists trust atoms? Because they make up everything! 😂\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_community.llms import Ollama \n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "\n",
    "if MODEL.startswith('gpt'):\n",
    "    model= ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=MODEL)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "else:\n",
    "    model= Ollama(model=MODEL)\n",
    "    embeddings = OllamaEmbeddings()\n",
    "model.invoke(\"Tell me a joke\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'During the COVID-19 pandemic, which started in 2020 and lasted for several months, there was no Major League Baseball (MLB) World Series played. The MLB season was cancelled in March 2020 due to the pandemic, and the World Series was not held that year.\\n\\nThe COVID-19 pandemic had a significant impact on sports events worldwide, including MLB, as many countries implemented lockdowns and travel restrictions to slow the spread of the virus. As a result, many sporting events were cancelled or postponed, including the MLB World Series.\\n\\nIn 2021, the MLB season resumed with a shortened schedule and reduced attendance at games due to ongoing pandemic concerns. The Los Angeles Dodgers went on to win the World Series that year, defeating the Tampa Bay Rays in the Fall Classic.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser\n",
    "chain.invoke(\"What MLB team won the World Series during the COVID-19 pandemic?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 0}, page_content='Synergy in Analytics: \\nUnifying Azure Databricks \\nand Microsoft Fabric'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 1}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  2\\nSynergy in Analytics: \\nUnifying Azure Databricks \\nand Microsoft Fabric\\n3 / \\nEmpower modern data analytics with \\nAzure Databricks and Microsoft Fabric \\n4 / \\nSimplify analytics workloads with Azure \\nDatabricks and Microsoft Fabric\\n10 / \\nMedallion architecture in Azure Databricks \\nand Microsoft Fabric\\n15 / \\nUse lakehouse data with Azure Databricks \\nand Microsoft Fabric\\n22 / \\nBetter together: Azure Databricks, Unity \\nCatalogue and Microsoft Fabric Purview 26 / \\nData Factory and Azure Databricks activity \\nin Microsoft Fabric\\n28 / \\nEnhance organisational capabilities with \\ngenerative AI\\n33 / \\nExplore real-world use cases with \\nhands -on examples\\n42 / \\nAchieve excellence with Azure Databricks \\nand Microsoft Fabric\\n43 / \\nNext steps'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 2}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  3\\nEmpower modern data analytics \\nwith Azure Databricks and \\nMicrosoft Fabric \\nToday, organisations need to understand \\nhow to manage the ever-increasing flood \\nof data in an efficient and insightful \\nmanner. A data lakehouse combines the \\nvast storage of a data lake with the \\nstructured processing of various data \\nservices. It supports extensive data storage \\nand complex analytics without \\ncompromise. It is more than a storage \\nsolution; it enhances data intelligence and \\nsupports advanced analytics, addressing \\nthe challenge of converting abundant data \\ninto actionable insights.\\nCloud environments offer vast \\ncomputational resources and scalability on \\ndemand; as an organisation’s data grows, \\ntheir infrastructure can grow alongside it \\nseamlessly and cost -effectively. This \\nsynergy between cloud platforms and the \\ndata lakehouse architecture is pivotal, \\nproviding a resilient and adaptable \\nfoundation for any enterprise looking to \\nthrive in the data -driven economy. \\nEffective management and robust security \\nmeasures are essential in the cloud-based \\ndata landscape to protect this \\ninvaluable  asset.Both Azure Databricks and Microsoft \\nFabric are comprehensive analytics \\nsolutions. Fabric has more \\nbusiness -user- friendly tools, and Azure \\nDatabricks has an integrated AI platform, \\nbut since they both rely on the same data \\nlayer, they can be used together as a more \\npowerful whole. Azure Databricks, Fabric \\nand OneLake allow organisations to \\nstreamline their data architecture, \\nsimplifying analytics workloads and \\nenabling efficient data management \\nand analysis across a unified platform.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 3}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  4\\nSimplify analytics workloads \\nwith Azure Databricks and \\nMicrosoft Fabric\\nThe modern data lakehouse architecture enables enterprises to utilise the synergies between \\nAzure Databricks and Microsoft Fabric. Both Azure Databricks and Fabric offer a unified, \\ncomprehensive set of tools for a broad spectrum of advanced analytics scenarios and work \\ntogether to provide a complete range of solutions for working with a data lakehouse. With \\nelements from data engineering, data science, data warehousing and Power BI, they deliver \\nwide-ranging analytics features, a cohesive experience for users and a single data repository \\naccessible to various analytics tools. Azure Databricks also provides comprehensive governance \\nand lineage tracking of both data and AI assets in a single unified experience.\\nFigure 1: Azure Databricks and Microsoft Fabric integration in a lakehouse architecture'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 4}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  5\\nMaximise data potential with \\nAzure Databricks and Microsoft \\nFabric\\nAzure Databricks and Fabric integration \\nallows users to seamlessly switch between \\nplatforms, offering customers a cohesive \\nand powerful solution for data \\nmanagement and analytics and facilitating \\nAI and machine learning projects with ease \\nand efficiency.\\nData management\\nOneLake centralises data from diverse \\nsources. The integration of Azure Databricks \\nwith Fabric not only revolutionises data \\nmanagement, scalability and data \\nprocessing but also centralises data from a \\nwide range of sources through OneLake. \\nThis comprehensive approach ensures that \\nAzure Databricks can seamlessly connect \\nwith data stored in Azure Data Lake Storage \\n(ADLS), various databases and OneLake \\nitself. This simplifies the management of \\nvast data volumes, enhances the ability to \\nscale data projects and streamlines the data \\nprocessing pipeline.• Centralised storage : Using OneLake \\nwithin Fabric allows centralised data \\nmanagement, which simplifies data \\naccess and governance, ensuring Azure \\nDatabricks can directly utilise the data \\nfor analytical processes. \\n• Seamless integration : The seamless \\nintegration between Azure Databricks \\nand Data Factory in Fabric facilitates \\nstreamlined workflows from data \\ningestion and validation to \\ntransformation. This integration enables \\na cohesive data management strategy \\nthat supports data analytics, data \\nscience, as well as AI projects. \\n• Enhanced security and accessibility : \\nPremium Azure Databricks workspaces \\nsupport credential passthrough, \\nstrengthening the security and ease of \\naccess to OneLake resources. This \\nfeature ensures secure and \\nstraightforward access to the centralised \\ndata for further processing and analysis.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 5}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  6\\nScalability and reproducibility\\nAzure Databricks and Fabric support \\nscalable data workflows, reproducible AI \\nand analytics projects and dynamic data \\nprocessing capabilities. This integration \\nallows organisations to manage large \\nvolumes of data efficiently, ensure \\nconsistent results across their data \\nenvironments and adapt processing power \\nto meet diverse requirements, driving \\nreliable and scalable data operations.\\n• Scalable data workflows : Azure \\nDatabricks activities within Data Factory \\nare designed to support scalable data \\nworkflows. Organisations can efficiently \\nhandle vast amounts of data, scaling \\ntheir data processing and analytics \\noperations as needed without \\ncompromising performance or reliability.\\n• Reproducible AI projects : The \\nintegration ensures that AI and analytics \\nprojects are reproducible, benefiting \\nfrom features such as data versioning \\nand lineage tracking. These features, \\navailable natively on both platforms, \\nenhance the reliability of AI projects \\nand ensure consistency across \\ndata  environments.• Dynamic data processing \\ncapabilities : Azure Databricks offers \\ndynamic data processing capabilities \\nthat adapt to varying data volumes and \\nprocessing requirements. This flexibility \\nis crucial for organisations to scale their \\ndata analytics operations efficiently.\\nData processing\\nCombining the robust data transformation \\ncapabilities of Azure Databricks with the \\nsophisticated orchestration of Data Factory \\npipelines, these technologies synergise to \\nsupport a comprehensive range of data \\nprocessing tasks. \\n• Efficient data transformation : Azure \\nDatabricks excels at transforming data \\nstored in OneLake and other sources. \\nTogether, Azure Databricks and Fabric \\nsupport an extensive range of data \\nprocessing tasks, including data \\nexploration, cleaning and preparation, \\ncrucial for preparing datasets for AI and \\nmachine learning.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 6}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  7\\n• Orchestration of complex workflows : \\nData Factory pipelines that include \\nAzure Databricks activities allow for the \\norchestration of complex data \\ntransformation workflows. These \\npipelines can validate data sources, copy \\ndata to designated storage and execute \\nnotebooks for data transformation, \\nproviding a comprehensive solution for \\ndata processing.  \\nThe synergy between Azure Databricks \\nand Fabric, especially through Data Factory, \\nenhances data management, ensures \\nscalability and reproducibility and \\nfacilitates efficient data processing. This \\nintegration is vital for organisations using \\ntheir data for insightful analytics and \\nAI-driven decision making.\\nAdvantages of lakehouse \\narchitecture\\nThe lakehouse architecture is built on the \\nopen-source Delta Lake storage format. In \\naddition to its technical capabilities, such as \\nACID transaction consistency, it enhances \\nthe overall effectiveness of the entire \\nplatform. Further, it enables use by multiple \\nprocessing engines at the same time as it \\nuses open formats and allows tools such as \\nAzure Databricks and Fabric to work with \\nthe same copy of the data at the same time. Enterprises don’t have to rely on just one \\ntool to process their data; instead, they can \\nselect the best tool for each project. \\nLakehouse architecture revolutionises how \\nenterprises manage, scale and process \\ntheir data. This innovative approach \\nensures data integrity and consistency \\nthrough transactional support, fostering a \\nmore effective data management \\nplatform. Furthermore, the architecture’s \\nability to amalgamate different storage \\nformats under one roof simplifies the \\ncomplex landscape of data estates. It \\nfacilitates the dynamic adjustment of \\ncomputing resources in line with real-time \\ndemand, eliminating wasteful \\nover -provisioning and enhancing cost \\nefficiency and resource utilisation.\\nAt the heart of this architecture is the \\nintegration of extensive data lakes and \\nstructured data warehouses, creating an \\noptimal environment for fostering AI and \\nmachine learning innovation. This ensures \\naccess to both computational power and \\ndata, accelerating innovation and \\nstreamlining the management of diverse \\ndata systems.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 7}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  8\\nOpen and governed data lakehouse\\nHybrid and multicloud data sourcesSecurity  |  Governance  |  Compliance\\nBusiness \\nIntelligenceReal-time and \\nstreaming \\nanalyticsData \\nScienceData \\nWarehouseData \\nEngineering\\nData \\nIntegration\\nData lake\\nAppliances Cloud DW Databases Hadoop …\\nFigure 2: Analytics in the lakehouse\\nWith the unique combination of the vast storage capacity of a data lake and the structured, \\nquery-optimised environment of a data warehouse, the modern lakehouse emerges as the ideal \\nplatform for developing and deploying AI algorithms. This dual capability ensures that AI projects \\ncan utilise the necessary computational power and data accessibility, speeding up innovation and \\nreducing overhead costs associated with managing separate data systems.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 8}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  9\\nBy simplifying data architecture and \\nreducing infrastructure complexity, \\nbusinesses can focus on creating value \\nthrough AI rather than grappling with \\ndata  management challenges in the \\nfollowing ways:\\n• The lakehouse architecture stores Delta \\nLake files in an ADLS account. This cloud \\nstorage service is extremely cost \\neffective, and the Delta Lake format \\nallows the storage of both structured \\nand unstructured data. \\n• Building AI models requires massive \\namounts of compute power from both \\ntraditional CPUs and advanced GPUs. \\nSince the data lakehouse architecture \\nempowers the use of multiple compute \\nengines, including both Azure \\nDatabricks and Fabric, enterprises can \\nbring the right type of processing \\npower to their data exploration and \\ndata modelling tasks.Enterprises can use the advanced machine \\nlearning and AI capabilities of Azure \\nDatabricks and Fabric on their full data \\nestate stored in a data lakehouse. These \\ntools include end-to-end experiment \\nmanagement and automated machine \\nlearning toolkits that can super-charge \\nAI projects.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 9}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  10\\nMedallion architecture in Azure \\nDatabricks and Microsoft Fabric\\nThe medallion architecture is a sophisticated approach within the broader concept of the \\nlakehouse architecture, designed to streamline data workflows from ingestion to insights. \\nAt its core, it consists of three layers: bronze, silver and gold, each serving a distinct purpose \\nin the data lifecycle.\\nFigure 3: Medallion architecture'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 10}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  11\\nThe three layers of the medallion \\narchitecture are:\\n1. Bronze (raw) : In this layer, raw data is \\ninitially ingested, retaining its original \\nform. It acts as a staging area and is \\ncrucial for capturing the full granularity \\nof data without any loss of fidelity.\\n2. Silver (validated) : In this layer, data \\nfrom disparate sources is matched, \\nmerged and conformed, making it \\nready for more complex analytical tasks. \\nThe silver layer is designed to provide \\nan enterprise view of key business \\nentities and is critical for supporting \\nself-service analytics and intermediate \\ndata storage needs. \\n3. Gold (enriched) : In this layer, data is \\nfurther optimised for specific business \\nneeds and is often structured into \\nde-normalised, read-optimised formats \\nthat are suitable for high-performance \\nquerying and reporting. The gold layer \\ntypically hosts data models that are \\ndirectly used in business intelligence \\napplications and decision support \\nsystems. Data becomes a true business \\nasset in the gold layer, offering valuable \\nand actionable insights.Azure Databricks and Fabric utilise this \\narchitecture to enhance their data \\nmanagement and analytical offerings. \\nTogether, they create a robust \\nenvironment in which data not only flows \\nseamlessly through each stage of the \\nmedallion architecture, but is also enriched \\nand made more accessible. \\nLakehouse integration \\nwith Azure Databricks and \\nMicrosoft Fabric\\nThe lakehouse-first pattern represents a \\ntransformative approach to data \\nmanagement and analytics. This approach \\nis built on a tiered data storage system, \\norganising data into bronze, silver and \\ngold layers. This structured flow of data \\nfacilitates more efficient data processing, \\nanalytics and machine learning \\napplications to transform raw data into \\ndata that’s optimised for businesses.\\nAzure Databricks excels in processing \\nlarge volumes of data with its Spark-based \\nanalytics engine, effectively handling \\ndata  transformations necessary for \\ntransitioning data from the bronze layer \\nto the silver layer.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 11}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  12\\nFabric provides a cohesive analytics \\nplatform that integrates deeply with Azure \\nDatabricks. It offers sophisticated data \\nmanagement tools and helps connect and \\ningest data seamlessly from various sources \\nthrough its extensive connector ecosystem. \\nThis integration ensures that data moves \\nfreely through each layer of the medallion \\narchitecture, maintaining integrity and \\nconsistency while minimising complexity \\nand overhead.\\nFoundation of open-source \\nstorage formats\\nAdopting Apache Parquet and Delta Lake \\nenables OneLake and Azure Databricks to \\noptimise Fabric engines and enhance \\ninteroperability across their platforms. This \\nstrategy ensures the robust handling of \\nlarge datasets, facilitates seamless data \\naccess across the lakehouse architecture \\nand reduces the complexities typically \\nassociated with managing large-scale data \\narchitectures:\\n• Standardisation on Apache Parquet \\nand Delta Lake : OneLake adopts these \\nformats for handling large datasets and \\nsupport for transactional capabilities \\n(ACID properties). This standardisation \\nensures that all data across the Fabric \\nengines is optimised for both \\nperformance and compatibility, \\nleading to more efficient data \\nprocessing workflows.• Optimisation of Fabric engines for \\nApache Parquet and Delta Lake : By \\nredesigning data processing engines to \\nbe optimised for these formats, the \\nsystem ensures high-performance data \\noperations, which are crucial for \\nprocessing large volumes of data \\nefficiently.\\n• Interoperability across the system : \\nThe ability of Azure Databricks to read \\nany Fabric artifact in OneLake highlights \\nthe interoperable nature of these \\ntechnologies, ensuring that data can be \\nseamlessly accessed and utilised across \\ndifferent parts of the lakehouse \\narchitecture.\\nThe use of open-source storage formats \\nwithin a lakehouse architecture maximises \\ndata utility, increases operational efficiency \\nand reduces the complexity that’s \\ntraditionally associated with managing \\nlarge-scale data architectures. Apache \\nParquet and Delta Lake facilitate this by \\nensuring that data is stored in a robust and \\nwidely compatible format, making it easier \\nfor organisations to integrate and analyse \\ndata across diverse systems and platforms.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 12}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  13\\nIntegration of open-source \\nformats with medallion \\narchitecture\\nThe use of open-source formats such as \\nApache Parquet and Delta Lake \\nstandardises data storage and access \\nwithin this integrated system, supporting \\nadvanced data management features such \\nas ACID transactions and schema \\nevolution. The powerful combination of \\nprocessing capabilities of Azure Databricks \\nand the management tools of Fabric within \\nthe medallion architecture enables \\nbusinesses to approach data architecture \\nin a scalable, efficient and highly conducive \\nmanner, facilitating the generation of \\ntransformative insights.\\nAzure Databricks excels in data processing \\nand analytics, using Apache Spark to \\nperform robust data transformations and \\nanalyses at scale. Its integration with ADLS \\nGen2 allows Azure Databricks to handle \\nmassive datasets efficiently, preparing data \\nfor further analytical processing. Fabric \\nextends the capabilities of Azure \\nDatabricks by offering additional tools for \\ndata management, such as easy access to \\ndata sources through more than 200 \\nnative connectors and streamlined data \\ningestion mechanisms. This allows \\nenterprises to implement a comprehensive \\ndata strategy that covers everything from \\ningestion to insightful analytics.Within this architecture, OneLake helps \\ncentralise data management without \\nnecessitating physical data movement. \\nData stored in various locations can be \\naccessed and analysed as if it were within \\na single repository. The federation \\ncapabilities of Azure Databricks further \\ncomplement this by allowing queries \\nacross different data stores, thereby \\nenhancing the flexibility and scope of \\ndata  analytics.\\nThe synergy between Azure Databricks \\nand Fabric provides a robust foundation \\nfor building advanced lakehouse \\narchitectures. This combination simplifies \\ndata management across disparate data \\nsources and enhances the analytical \\ncapabilities of organisations, enabling \\nthem to derive actionable insights more \\nefficiently and with greater accuracy. This \\nempowers enterprises with the ability to \\nmaximise the value of their data assets in \\na secure and scalable manner.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 13}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  14\\nLakehouse-first pattern \\nscenarios\\nBy adopting the medallion architecture \\nwith Azure Databricks and Fabric, \\nintegrating lakehouse data with Fabric \\nworkloads and utilising OneLake data with \\nLakehouse Federation, organisations can \\nenable a sophisticated, tiered storage \\nmodel within a lakehouse environment. \\nThis unified approach facilitates efficient \\ndata processing and management. \\nIt supports agile responses to data-driven \\ninsights and operational needs while \\nenhancing the scalability and flexibility \\nof data analytics across multiple \\nstorage  systems:\\n• Medallion architecture with Azure \\nDatabricks and Fabric : This approach \\nutilises a tiered storage model within a \\ndata lakehouse environment, facilitating \\nefficient data processing and \\nmanagement. By using Azure \\nDatabricks in conjunction with Fabric, \\norganisations can manage large-scale \\ndata analytics pipelines more effectively.• Integrating Azure Databricks \\nlakehouse data with Fabric \\nworkloads : In this scenario, data stored \\nand managed in an Azure Databricks \\nlakehouse can be directly used with \\nthe analytics tools in Fabric. This \\nintegration supports a more agile \\nresponse to data-driven insights and \\noperational  needs.\\n• Utilising OneLake data in Azure \\nDatabricks with Lakehouse \\nFederation : This set-up allows the \\nutilisation of data across multiple \\nstorage systems within Azure \\nDatabricks. By federating data sources, \\nusers can query data across these \\nsources as if they were a single entity, \\nenhancing the flexibility and scalability \\nof data analytics operations.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 14}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  15\\nUse lakehouse data with Azure \\nDatabricks and Microsoft Fabric\\nThe synergy between Azure Databricks \\nand Microsoft Fabric offers organisations a \\npowerful and efficient way to handle their \\ndata workloads. From ingestion and \\nstorage to analysis and reporting, \\nenterprises benefit from a secure and \\ngoverned framework. This flexibility \\nempowers teams to select the platform \\nthat best fits their project needs, ensuring \\nseamless integration within the broader \\nenterprise ecosystem.\\nFor example, a data science team that \\nworks primarily in notebook coding \\nenvironments will appreciate the rich \\nfeatures of the Azure Databricks UI and the \\nflexibility for managing advanced Spark \\nlibraries on the clusters, and AI engineers \\nwill appreciate the native ability to \\nfine-tune models on their data. Business \\nanalysts may prefer the ease of use of the \\nlow-code dataflows in Fabric for quickly \\nbuilding pipelines that transform data and \\ncreate new datasets in the gold layer of the \\nlakehouse. Both teams can use their \\npreferred tools to work on the same \\ndatasets without the need for either \\nteam to make copies of the data in their \\nown environment.Interacting with \\nlakehouse  data\\nLakehouse data within the Azure \\necosystem is typically stored in cloud \\nlocations, which can be categorised into \\ntwo primary types:\\n1. ADLS accounts : ADLS is a cloud \\nstorage system optimised for analytics \\nworkloads. Enterprises can create and \\nmanage ADLS accounts to suit their \\ndata administration needs.\\n2. OneLake : OneLake is also an ADLS \\naccount, but unlike other accounts, \\nAzure customers do not directly \\nmanage it. Instead, it is created as part \\nof and administered by Fabric. It does \\nnot appear in the Azure portal, and \\nalthough customers can interact with \\nthe data it contains, they do not have \\nmuch control over the account itself.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 15}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  16\\nWarehousing\\nIngest, ETL and \\nreal-time processingAI\\nUC: Governance, \\nsharing, integration\\nFigure 4: Linking ADLS accounts to OneLake with shortcuts\\nOneLake introduces shortcuts to help data \\nprofessionals access data across OneLake \\nand multiple ADLS accounts. Shortcuts \\nallow data professionals to link data in \\ntheir Unity Catalogue (with Azure \\nDatabricks shortcuts) or external ADLS \\naccounts to OneLake, making them appear \\nunified. Users can access data from these \\naccounts seamlessly, without realising \\nthey’re from different sources. Shortcuts \\nhelp efficiently manage data by virtualising \\naccess to external data sources without \\nunnecessary duplication, supporting \\nscalable and efficient AI model training \\nand deployment processes.Integrate Azure Databricks \\nwith Power BI for enhanced \\ndata visualisation\\nFor advanced visualisation and \\ndashboarding scenarios with an Azure \\ndata lakehouse, most enterprises choose \\nMicrosoft Power BI as their tool of choice. \\nThis powerful visualisation and analytics \\ntool is now offered as part of Fabric, \\nallowing enterprises to fully integrate the \\nadministration and billing of Power BI with \\nother Fabric resources.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 16}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  17\\nAzure Databricks integrates seamlessly \\nwith Power BI. Databricks SQL warehouses \\nand Unity Catalogue offer a flexible and \\nscalable solution for Power BI in the \\nlakehouse. Data that has been processed \\nby Azure Databricks can be used in \\nthree  ways:\\n1. Azure Databricks Direct Publish for \\nPower BI : Databricks can now \\nautomatically sync tables, including \\nrelationships, to Power BI semantic \\nmodels in a single click. This helps \\nanalysts build reports and dashboards \\nfaster than ever before.\\n2. Azure Databricks connector in Power \\nBI Desktop : Azure Databricks allows \\nthe Power BI client to connect to an \\nAzure Databricks cluster, which can \\nquery and process the lakehouse data \\nfor you and send the results to Power BI \\nfor visualisation.\\n3. Power BI Direct Lake mode : Power BI \\ncan use its new Direct Lake mode to \\ndirectly read Delta Lake data that has \\nbeen written to an Azure storage \\nlocation. This can be data that was \\nwritten by Azure Databricks or by \\nFabric, and the storage location can be \\nthe OneLake account or any other \\nADLS  account. The previous section detailed how to \\nuse Azure Databricks to process raw data, \\nprepare it for reporting and then write \\nit to the lakehouse.\\nDirect Lake mode in Power \\nBI to read and visualise \\nAzure  Databricks  data\\nWith OneLake storage, the files are stored \\nin the efficient Delta Lake format. These \\nDelta Lake tables have been optimised by \\nthe VertiPaq engine, making them highly \\nefficient for consumption by Power BI. This \\nenables Power BI to directly interact with \\nthe Delta Lake tables stored in OneLake \\nwithout the need for an intermediary \\ncaching layer, such as Azure Analysis \\nServices or Power BI datasets. This new \\nmode of access, called Direct Lake mode, \\nprovides real -time data access without the \\nneed for refreshing models in Power BI.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 17}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  18\\nDirectly publish datasets \\nto Power BI workspaces:\\n● Publish from Azure UI, without \\nPower BI Desktop\\n● Publish entire schemas with table \\nrelationships (PK/FK)\\nFigure 5: Deep Power BI integration\\nThe default dataset includes all tables from the lakehouse, allowing users to establish \\nrelationships and apply various modelling changes. These datasets from Unity Catalogue can \\nbe directly published to Power BI. Users can access and edit a published semantic model with \\nthe web modelling editor that’s accessible through Power BI.\\nIn the model view within the web modelling editor, you can see whether there’s a Direct Lake \\nconnection by hovering the cursor over the table headers. Direct Lake also allows for the \\ncreation of new Power BI datasets directly through the web. This process ensures the use of \\nDirect Lake for the connection. To learn more about using the web editor for semantic models, \\nthe following document will help you get started: Edit data models in the Power BI service \\n(preview) – Power BI | Microsoft Learn .'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 18}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  19\\nIntegrate Azure Databricks \\nwith Power BI to enhance \\ndata  workflows\\nIntegrating Azure Databricks with Power BI \\nprovides significant advantages for data \\nmanagement and visualisation, enhancing \\nboth security and performance in data \\nanalytics workflows:\\n1. First, the integration allows more secure \\nand interactive data visualisation \\nexperiences directly from the data lake, \\navoiding the latency and costs \\nassociated with traditional data \\nprocessing workflows. It uses Microsoft \\nEntra ID for authentication, simplifying \\nthe user experience and increasing \\nsecurity, eliminating the need for \\npersonal access tokens. This integration \\nensures that security controls at the \\ndata lake level are enforced within \\nPower BI, maintaining consistent \\nsecurity policies across platforms.1\\n2. Second, the semantic lakehouse \\narchitecture streamlines data ingestion \\nand storage. It provides a unified \\nstorage layer that supports an extensive \\nrange of data formats and structures, \\nsignificantly boosting the efficiency of \\ndata processing and transformation. \\n1 Power Up your BI with Microsoft Power BI and \\nLakehouse in Azure Databricks: Part 1 – Essentials – \\nMicrosoft Community HubThis set-up not only simplifies the \\nanalytics stack, but also enhances data \\nquality and accessibility for BI tools, \\nenabling more sophisticated data \\nmodelling and analytics directly on \\nlarge  datasets.2 \\n3. Last, the integration supports advanced \\nanalytics scenarios, simplifying the \\nmanagement and analysis of large data \\nvolumes. The DirectQuery option in \\nPower BI plays a crucial role here, \\nallowing users to perform real-time \\nanalysis without moving data out of the \\nlakehouse. This capability is critical for \\nmaintaining up-to-the-minute accuracy \\nin reports and dashboards, providing \\nbusinesses with insights that are both \\ndeep and immediately actionable.3\\nThese features collectively make Azure \\nDatabricks and Power BI a robust \\ncombination, offering businesses \\nadvanced tools to harness their data \\neffectively and securely. To learn more \\nabout Power BI and Azure lakehouse \\nintegration, consult the following \\ndocument: https://learn.microsoft.com/\\nfabric/get-started/directlake-overview  \\n2 Power Up with Power BI and Lakehouse in Azure \\nDatabricks: Part 3 – Tuning Azure Databricks SQL – \\nMicrosoft Community Hub\\n3 https://docs.databricks.com/partners/bi/power-bi.html'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 19}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  20\\nUse Data Activator to alert on \\nchanges in Azure Databricks \\ndata via Power BI\\nA common scenario for enterprises is that \\nthey want to receive alerts if particular \\nmetrics exceed certain thresholds. For \\nexample, they may want to know if there’s \\na sudden, unexpected spike in sales for a \\nparticular item, or they may want to know \\nif the volume of transactions has \\nplummeted below the normal range, \\nindicating a possible problem in the \\ntransaction pipeline.\\nThese scenarios can be handled by a new \\nfeature in Fabric called Data Activator. This \\nno-code tool monitors data in a Power BI \\nreport and automatically takes action if the \\ndata matches certain patterns or hits \\nspecified thresholds. When these events \\noccur, Data Activator can take an action \\nsuch as alerting a user or launching a \\nPower Automate workflow.\\nIn order to enable Data Activator, please \\nfollow the official documentation here: \\nhttps://learn.microsoft.com/fabric/\\ndata-activator/\\nTo create an alert with Data Activator \\nwhen a freezer’s temperature falls below \\n30° F in a Power BI report, follow these \\nsteps for monitoring freezer temperatures \\nwithin a Fabric workspace:1. Confirm that your Power BI report, \\nwhich includes freezer temperature data, \\nis published online to a Fabric workspace \\nequipped with Premium capacity.\\n2. Choose the temperature visual:\\na. Access the report : Open the specific \\nPower BI report that tracks freezer \\ntemperatures.\\nb. Select the relevant visual : Find \\nthe visual that displays the freezer \\ntemperatures.\\n3. Click the ellipsis (…) in the top-right \\ncorner of the temperature visual and \\nselect Set Alert  or use the Set Alert  \\nbutton found in the Power BI toolbar.\\n4. In the Set Alert  pane, specify how you \\nwish to receive alerts (email or Teams). \\nIf your visual includes multiple freezers \\n(dimensions), use the For each  \\ndropdown to select the specific \\ndimension (freezer) to monitor.\\n5. Define the alert condition, such as when \\nthe temperature drops below 30° F. Data \\nActivator will monitor the temperature \\nand notify you when this condition \\nis met.\\n6. Decide where to save your Data \\nActivator trigger in Power BI. You can \\nadd it to an existing reflex item or create \\na new one.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 20}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  21\\n7. Click Create alert  to finalise your Data \\nActivator trigger. You can optionally \\ndeselect Start my alert  if you prefer to \\nedit the trigger in Data Activator before \\nactivating it.\\nBy following these steps, you’ve \\nsuccessfully set up an alert in Data \\nActivator to notify you when a monitored \\nfreezer’s temperature falls below 30° F, \\nallowing you to take immediate action if \\nnecessary. Once these data updates are \\ncomplete, you should receive the alert \\nfrom Data Activator that was configured.\\nUse Lakehouse Monitoring \\nwith Alerts to alert on changes \\nin Azure Databricks\\nEnterprises often require alerts when data \\nquality metrics exceed certain thresholds. \\nFor example, they may want to know if \\nthere’s a sudden, unexpected spike in the \\nnumber of missing values within a \\nparticular field, indicating a possible \\nproblem in the transaction pipeline, or if \\nthe quality of predictions from a machine \\nlearning model has declined, indicating a \\nneed to retrain the model on newer data.\\nThese scenarios can be handled with an \\nAzure Databricks feature called Lakehouse \\nMonitoring with Alerts. This no-code tool \\nmonitors data quality in Unity Catalogue and automatically takes action if the data \\nmatches certain conditions or exceeds \\nthresholds. When these events occur, \\nAlerts will take a specified action, such as \\nsending a notification via email, Slack or \\nTeams. The alert can also call a webhook \\naction, allowing users to build extensible, \\ncustom workflows based on changes in \\nthe data.\\nA monitor is a process that runs on a \\nspecified schedule to check the data \\nquality of a particular table. When a user \\ncreates a monitor, it computes the data \\nquality metrics for the table and stores the \\ncurrent values in a separate system table. \\nEach time the monitor runs, it recomputes \\nthe quality metrics and compares them to \\nthe original values. If the quality has \\ndeteriorated, then an alert will be raised. \\nFor details on how a monitor can be \\ncreated, consult the following document: \\nhttps://docs.databricks.com/lakehouse-\\nmonitoring/create-monitor-ui.html\\nIf a monitor detects that the quality of the \\ndata in the table has declined, it will raise \\nthe specified alert. This can be used to \\nsend a notification to the data engineering \\nteams so they can investigate further. For \\ndetails on how these alerts can be \\nconfigured, check the following document: \\nhttps://docs.databricks.com/lakehouse-\\nmonitoring/monitor-alerts.html'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 21}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  22\\nBetter together: Azure \\nDatabricks, Unity Catalogue and \\nMicrosoft Fabric Purview \\nAs the demand for analytics grows and \\ndata platforms evolve into more intricate \\nsystems, governing the platform – \\nmanagement of data availability, usability, \\nintegrity and security – becomes \\nparamount. In a data lakehouse \\narchitecture, data governance helps ensure \\nthat data is properly catalogued, classified \\nand managed. By implementing effective \\ndata governance, organisations can \\nmanage their data properly and use it to \\ndrive business value.\\nEffective data governance in a data \\nlakehouse architecture requires the \\nimplementation of policies, procedures \\nand standards for managing data. This \\nincludes defining data ownership and \\nstewardship, establishing data quality \\nstandards and implementing data security \\nand compliance measures. To provide \\nthese crucial data governance capabilities, \\nboth Azure Databricks and Microsoft \\nFabric offer powerful, modern features.Unity Catalogue in Azure \\nDatabricks\\nAzure Databricks includes Unity Catalogue, \\nwhich provides centralised fine-grained \\naccess control for an organisation’s data \\nstorage locations, auditing of data access \\nand lineage tracking from ingestion to all \\ndata workloads and Azure Databricks \\nprovides column-level and row-level \\naccess controls and data discovery tools. \\nIt now also includes system tables, which \\nprovide a straightforward way to query \\naudit data, billing data and lineage. \\nAdditionally, Unity Catalogue is supported \\nby AI capabilities to automatically \\ndocument tables and columns, facilitate \\nsemantic search and help surface related \\ndata products.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 22}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  23\\nMicrosoft Purview to govern \\nMicrosoft Fabric\\nFabric integrates with Microsoft Purview \\nfor data governance, information \\nprotection and data loss prevention. The \\ninformation protection features enable \\nenterprises to discover, classify and protect \\nthe data stored in the lakehouse and apply \\nsensitivity labels to it. Data loss prevention \\nuses policies to detect when sensitive data \\nis uploaded to Power BI semantic models \\nor other supported Fabric assets. It can \\nalso help detect common sensitive data. \\nFabric also includes tools for discovering \\ndata lineage so that data can be tracked \\nthrough the analytical process as it moves \\nfrom its original source, through the \\nvarious transformations and into the \\nvarious reporting models.\\nMicrosoft Purview and Unity \\nCatalogue to streamline data \\ngovernance\\nMicrosoft Purview and Unity Catalogue \\nare two powerful tools designed to \\nenhance data governance and \\nmanagement within cloud environments, \\nparticularly for the users of the extensive \\ncloud services offered by Microsoft. The broad governance capabilities of \\nMicrosoft Purview can extend into the \\nAzure Databricks environment, where \\nUnity Catalogue applies specific \\ngovernance and security measures to \\nAzure Databricks workspaces. This \\nintegration allows organisations to \\nmaintain a consistent governance strategy \\nacross all platforms, enhancing security \\nand operational efficiency. Organisations \\ncan ensure that data policies are uniformly \\napplied, data lineage is clear and auditable \\nand all regulatory compliance \\nrequirements are met across their entire \\ndata estate.\\nUnity Catalogue offers a sophisticated and \\ncentralised governance solution for \\nmanaging a variety of data assets within \\nthe Azure Databricks lakehouse platform. \\nIt integrates seamlessly with Azure to \\nprovide fine-grained governance \\ncapabilities, including access control, \\nauditing and data lineage. Unity Catalogue \\nsimplifies data management across \\nmultiple Azure Databricks workspaces, \\nenabling organisations to enforce \\nconsistent security and compliance policies \\nacross their data assets, whether they are \\nfiles, tables or machine learning models.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 23}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  24\\nUnity Catalogue provides a single point of \\ncontrol for data access policies, which \\napply uniformly across all workspaces. This \\nensures that data governance is not only \\ncentralised, but also deeply integrated into \\nthe Azure Databricks environment, \\nenhancing security and governance. \\nAdditionally, Unity Catalogue supports \\ncomprehensive data discovery, making it \\neasier for users to find and access the data \\nthey need while adhering to the defined \\naccess controls and policies. This unified \\napproach helps streamline operations and \\nreduces the complexity typically associated \\nwith managing large and diverse \\ndata  environments.\\nMicrosoft Purview allows enterprises to \\nmaintain control over their data through \\nFabric, enabling seamless integration and \\nmanagement of data from various sources \\ndown to detailed reports. Along with a \\nsuite of tools to protect sensitive data \\nacross different environments, Microsoft \\nPurview provides capabilities such as \\nsensitive data discovery, classification and \\nprotection using sensitivity labels. It also \\nfacilitates comprehensive auditing and \\ndata loss prevention strategies specifically \\ntailored for complex environments such as \\nPower BI semantic models. Best practices \\nThe integration of Azure Databricks with \\nMicrosoft Purview focuses on maximising \\ndata governance and security within Azure \\nDatabricks environments. Key best \\npractices for this integration include:\\n• Secure access to critical data : \\nMicrosoft Purview can be used to \\nautomatically discover and classify data \\nwithin Azure Databricks, visualise data \\nlineage and manage access controls \\neffectively. This ensures that only \\napproved personnel can access sensitive \\nor critical data and that all data policies \\nare consistently applied across \\nAzure  services.\\n• Use two separate connectors to \\nmanage metadata : Microsoft Purview \\noffers two separate connectors for \\nAzure Databricks. Most enterprises will \\nuse the Azure Databricks Unity \\nCatalogue connector because Unity \\nCatalogue enables many of the modern \\nfeatures in Azure Databricks. However, \\nfor customers who have not yet \\nmigrated to Unity Catalogue and are \\nstill using Hive to manage their \\nmetadata, Microsoft Purview also has \\nan Azure Databricks Hive Metastore \\nconnector that can be used.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 24}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  25\\n• Utilise custom rule sets : Enterprises \\ncan use Microsoft Purview to scan \\ncatalogues, schemas, tables and views. \\nAs a best practice, enterprises should \\nuse custom rule sets in addition to the \\nrule sets provided by Microsoft Purview. \\nCreating a custom rule set for different \\nregions of the world can speed up the \\nscanning process by using only the \\nclassification rules required in a \\nparticular region.\\n• Indicate data sensitivity with \\nlabelling tools : Labelling tools in \\nMicrosoft Purview can be used on Unity \\nCatalogue data to indicate the \\nsensitivity of files and data columns. \\nThese labels travel with the data and \\ncan be used by other tools in the \\nMicrosoft data ecosystem, such as \\nSharePoint and Power BI, to \\nautomatically apply data handling \\npolicies.The combined capabilities of Microsoft \\nPurview, Azure security in OneLake and \\nUnity Catalogue support a resilient and \\nagile data governance strategy, enabling \\nbusinesses to use their data assets \\neffectively in a digital landscape.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 25}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  26\\nData Factory and Azure \\nDatabricks activity in \\nMicrosoft Fabric\\nAzure Databricks activity in Microsoft \\nFabric represents a significant evolution in \\ndata processing within cloud environments, \\nintegrating the extensive capabilities of \\nAzure Data Factory into a more unified \\nand robust framework. With the new \\nAzure Databricks activity, users can easily \\ncreate and manage data pipelines in Fabric, \\nincorporating sophisticated analytics \\nand processing tasks directly into \\ntheir workflows. Users can configure Azure Databricks \\nclusters that are used for data processing \\ndirectly within Fabric, like the functionality \\noffered in Azure Data Factory. This \\nincludes the ability to set up Azure spot \\ninstances for accessing unused Azure \\ncompute capacity at reduced costs and \\nspecifying cluster policies to ensure that \\ncluster configurations meet organisational \\nstandards and requirements.\\nOne activity that encompasses all three job types: \\nNotebook, Jar, PythonUnity Catalog  support and \\nPolicy ID  integration\\nFigure 6: Seamless integration between Azure Databricks and Data Factory with Microsoft Fabric'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 26}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  27\\nMoreover, the enhanced Azure Databricks \\nintegration introduces several new \\nfeatures. They are:\\n• Configure Unity Catalogue access \\nmode : Users can configure the access \\nmode of Unity Catalogue, which \\nenhances governance and security by \\nmanaging permissions more \\nmeticulously. \\n• Run multiple tasks in a single \\nDatabricks activity : Users can run \\nvarious tasks, such as notebooks, JARs \\nand Python scripts, within a single \\nDatabricks activity, streamlining the \\nprocess and reducing the complexity \\npreviously associated with managing \\nmultiple types of data jobs.• Monitor notebook runs in real time : \\nDuring operation, users can initiate a \\ndata pipeline and immediately monitor \\nits execution, with the system providing \\ndirect links to the outputs in the Azure \\nDatabricks instance. This real-time \\nmonitoring capability allows users to \\ntrack the details of the notebook runs, \\nincluding cluster performance and \\ncomputational efficiency, directly \\nfrom  Fabric.\\nThis integration not only brings existing \\nAzure Databricks capabilities from Azure \\nData Factory into Fabric but also \\nintroduces new functionalities such as \\ncluster policy and Unity Catalogue \\nsupport, enhancing the overall data \\nmanagement and analytics experience.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 27}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  28\\nEnhance organisational \\ncapabilities with generative AI\\nAdvanced AI models have changed the technology landscape. Enterprises are trying to unlock \\nthe potential of their data and use AI to expand their business capabilities. This includes the use \\nof generative AI to build their data lakehouse on Azure. \\nThe next section will detail how to describe your desired architecture to ChatGPT and have it \\ngenerate code that can be implemented in Azure.\\nGenerate architecture and code with Azure OpenAI\\nUsing prompt engineering, you can generate architecture plans and code using the same process \\nthat would be used to summarise a technical article.\\nPrompt Input\\nI want you to act as a data engineer. I will provide details about \\na\\xa0scenario where a customer needs to utilise data stored in ADLS Gen2. \\nIt\\xa0will be accessed by Azure Databricks and Microsoft Fabric to load \\ndata for machine learning models. It will be your job to come up with \\nan architecture to address this need.\\nAnswer from Azure OpenAI\\nTo address the scenario where a customer needs to utilise data \\nstored\\xa0in Azure Data Lake Storage Gen2 (ADLS Gen2), accessed by Azure \\nDatabricks and Microsoft Fabric for loading data into machine learning \\nmodels, we’ll design an architecture that leverages these services \\nefficiently. This architecture aims to streamline data processing, \\nenhance data governance and ensure scalable machine learning \\noperations.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 28}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  29\\nArchitecture Overview\\n •Data storage layer: Azure Data Lake Storage Gen2: Acts as the \\ncentral repository for storing raw data, processed data and \\nmachine learning artefacts. ADLS Gen2 is optimised for large-scale \\nanalytics scenarios and supports hierarchical namespace, which \\nsimplifies data management.\\n •Data processing and analytics layer: Azure Databricks: Serves \\nas the primary platform for data processing, analysis and \\nmachine learning model development. Azure Databricks provides a \\ncollaborative environment with support for multiple programming \\nlanguages and integrates seamlessly with ADLS Gen2 for direct \\naccess to data.\\n •Data management and governance layer: Microsoft Fabric: Utilised \\nfor managing and governing the data across the entire data \\nlandscape. Microsoft Fabric allows creating a unified data \\ngovernance model that encompasses data in ADLS Gen2 and data \\nprocessed or generated by Azure Databricks.\\n •Machine learning operationalisation: Azure Machine Learning: \\nIntegrates with Azure Databricks for deploying and managing machine \\nlearning models. This service provides tools for model versioning, \\nmonitoring and serving, facilitating the operationalisation of \\nmachine learning models.\\nDetailed Workflow\\n1. Data ingestion: Data is ingested into ADLS Gen2 from various sources, \\nincluding structured databases, IoT devices, log files, etc. This \\ndata is stored in raw format within a hierarchical file system \\nstructure.\\nNote\\nThe rest of the content generated by Azure OpenAI can be found in the Appendix .  \\nPrompts and answers reflected here may not reflect your exact experience.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 29}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  30\\nGenerate and correct code \\nwith Databricks Assistant\\nDatabricks Assistant is an AI -powered tool \\nintegrated within the Azure Databricks \\nenvironment, designed to enhance coding \\nproductivity by assisting users with code \\ngeneration, error resolution and \\ndocumentation directly within Databricks \\nnotebooks. It uses the capabilities of Azure \\nAI services, enhancing the way developers \\ninteract with data and code within the \\nplatform. \\nDatabricks Assistant acts as a robust tool \\nthat supports developers by automating \\nroutine tasks, optimising code, explaining \\nfunctionalities and troubleshooting, all \\nwithin the Azure Databricks workspace. \\nThis not only speeds up the development \\nprocess but also helps in maintaining a \\nhigh standard of code quality and \\ndocumentation, making it an asset in data \\nengineering and analysis workflows. \\nDatabricks Assistant enhances productivity \\nin data science and engineering by utilising \\nAI to help with code generation, error \\nresolution and documentation within the \\nAzure Databricks environment in the \\nfollowing ways:\\n• Code generation : Databricks Assistant \\nsimplifies coding by allowing users to \\ninput their requirements in natural language. It can generate executable \\nSQL queries or transform code from one \\nlanguage to another, such as converting \\nPython pandas code to PySpark. This \\nfeature speeds up development and \\nreduces manual coding errors.\\n• Error resolution : Databricks Assistant \\ncan quickly identify and clarify coding \\nerrors, offering solutions by generating \\ncorrective code snippets. This is valuable \\nfor both novice and experienced \\nprogrammers, as it provides immediate \\nsolutions to common syntax and \\nruntime issues, thus minimising \\ndowntime.\\n• Documentation : It assists in \\ndocumenting code by automatically \\ngenerating comments that explain the \\nfunctionality of code blocks, supporting \\nthe maintenance of clean and \\nunderstandable codebases essential for \\nlong-term project sustainability and \\nteam collaboration.\\n• Contextual help and learning : \\nDatabricks Assistant aids in context by \\nunderstanding the user’s environment, \\nincluding frequently used tables, \\nschema and previous queries. It uses \\nthis context to provide precise answers \\nand tailored code snippets, enhancing \\nproject-specific support.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 30}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  31\\n• Visualisations and dashboards : In \\nvisualisation environments such as \\nLakeview, Databricks Assistant can \\ngenerate data visualisations based on \\nuser prompts, allowing the quick \\ncreation and iteration of visual displays \\nwithout needing deep technical \\nexpertise.\\n• Data ingestion and ETL processes : \\nDatabricks Assistant streamlines data \\ningestion and ETL tasks by automating \\ncode generation and optimisation, \\nexpediting the set-up and execution of \\ndata pipelines.\\n• Security and compliance : Azure \\nDatabricks ensures that interactions \\nwith Databricks Assistant are secure, \\nadhering to user permissions and data \\ngovernance policies, which makes it \\nsuitable for use in sensitive and \\nregulated environments.\\n• Integration and accessibility : \\nDatabricks Assistant is accessible from \\nnotebooks, SQL editors and file editors, \\nmaking it a versatile tool for various \\ndata tasks.\\n• Feedback and iteration : Users can \\nprovide feedback directly through the \\nplatform, which helps improve \\nDatabricks Assistant’s accuracy \\nand functionality. This ensures that the \\ntool evolves in response to user needs \\nand challenges.Azure Databricks workspaces can be \\nprovisioned with a free 14-day trial that \\nincludes access to Assistant, allowing \\npotential users to evaluate its capabilities \\nand integrate it into their workflows \\nwithout an initial investment.\\nHere are some examples of how Databricks \\nAssistant can be used in different scenarios \\nto enhance productivity and simplify tasks:\\n1. Code generation\\nScenario : A user needs to extract and \\nsummarise sales data by region from \\na DataFrame.\\nUser input: \\nGenerate a SQL query to sum \\nall sales by region from the \\nsales_data table.\\nResponse:\\nsqlCopy code\\nSELECT region, SUM(sales) AS \\ntotal_sales FROM sales_data \\nGROUP BY region; \\n \\nUtility : This helps the user quickly get \\nthe desired code without having to \\nmanually write the SQL query.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 31}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  32\\n2. Fixing errors\\nScenario : A user has written PySpark \\ncode that fails due to a syntax error.\\nUser input: \\nThis code isn’t working.  \\nCan you help fix it?\\nDiagnosis and fix : Databricks Assistant \\nidentifies a missing comma in a \\nDataFrame operation and suggests the \\ncorrected code with a change \\nhighlighted.\\nUtility : The user gets immediate \\nfeedback and correction suggestions, \\nspeeding up troubleshooting and \\nreducing frustration.\\n3. Code documentation\\nScenario : A developer wants to add \\ncomments to a complex function to \\nimprove code readability.\\nUser input: \\nCan you document this \\nfunction explaining  \\neach step?Response : Databricks Assistant adds \\ncomments before each significant line \\nor block of code explaining what it \\ndoes, such as initialising variables, error \\nhandling and logic flows.\\nUtility : Ensures that the code is \\nunderstandable for future reference or \\nfor other team members, enhancing \\nmaintainability.\\nThese examples illustrate the practical \\nbenefits of Databricks Assistant in \\nreal-world development environments, \\nstreamlining the coding process, \\nsimplifying error resolution and ensuring \\nthorough documentation.\\nNote\\nPrompts and answers reflected here may \\nnot reflect your exact experience.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 32}, page_content=\"Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  33\\nExplore real-world use cases \\nwith hands -on examples\\nIn earlier examples, you used Python \\ncode to read the data and aggregate it to \\nanswer some business questions. \\nThis section looks at an alternative to \\nPython code to read data and how AI can \\nbe used to allow business users to query \\nlakehouse data using English instead of \\na query language.\\nUse the English SDK for Spark \\nto write queries in Azure \\nDatabricks and Fabric\\nTo utilise the English SDK for Apache \\nSpark, the following requirements should \\nbe met:\\nNote\\nAzure Databricks recommends using GPT-4.\\n1. Install the English SDK package : \\nBegin by adding the SDK to your \\nenvironment. Use the %pip install \\npyspark-ai --upgrade  command in \\nyour notebook to ensure you have the \\nlatest version.2. Restart the Python kernel : After \\ninstallation, you need to restart the \\nPython kernel to apply the updates. \\nExecute dbutils.library.\\nrestartPython()  in a new cell to \\nreset the environment.\\n3. Set the OpenAI API key : Your API key \\nfrom OpenAI is necessary for \\nauthentication. Implement it by setting \\nan environment variable with the \\nos.environ['OPENAI_ API_ KEY'] \\n= '<your-openai-api-key>'  \\nPython code, replacing <your-\\nopenai-api-key>  with your actual \\nAPI key.\\n4. Activate the SDK : To use the SDK, \\nactivate it within your notebook. This \\ninvolves initialising the SDK with your \\npreferred language model (such as \\nGPT-4) and then activating it to start \\ninterpreting English queries.\\n5. Create a DataFrame : Use SQL queries \\nwithin the notebook to fetch data from \\nyour Azure Databricks workspace and \\nsave it as a DataFrame. This DataFrame \\nwill be the basis for your English queries.\"),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 33}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  34\\n6. Query using English : Finally, query the \\nDataFrame by asking questions in plain \\nEnglish. The SDK interprets these \\nquestions and executes the \\ncorresponding SQL queries, returning \\nthe results directly to your notebook.\\nAn example query using English with the \\nEnglish SDK for Apache Spark could be \\nsomething such as: \\nWhat was the average trip \\ndistance for each day during \\nthe month of January 2016? \\nPrint the averages to the \\nnearest tenth. \\nThis query demonstrates how plain English \\ncan be utilised to conduct data analysis \\nactivities, such as calculating averages \\nfrom a dataset, with the English SDK, \\nallowing Apache Spark to interpret and \\nexecute English-language instructions.\\nAnother example query using English for \\nthe English SDK for Apache Spark could be: \\nShow me the total revenue for \\neach product category in the \\nlast quarter.\\nThis type of query illustrates how users can \\nrequest specific financial metrics, such as \\ntotal revenue, broken down by categories, \\nover a defined period, such as the last quarter, using natural language.  \\nThis approach simplifies complex data \\nanalysis tasks into straightforward \\nEnglish  questions.\\nCreating a notebook in \\nMicrosoft Fabric\\nFabric notebooks are a key tool for crafting \\nApache Spark jobs and conducting \\nmachine learning experiments. With \\nsupport for advanced visualisations and \\nMarkdown text integration, it offers a \\nweb-based interactive platform that’s \\npopular among data scientists and \\nengineers for coding. Data scientists \\nrely on these notebooks to develop \\nand deploy  machine learning models, \\nincluding  experimentation, model \\ntracking and deployment phases. \\nFabric notebooks offer:\\n• Immediate usability with no \\nset-up  required\\n• An intuitive, low-code interface for data \\nexploration and processing\\n• Enhanced data security through \\nintegrated enterprise-level features\\n• The ability to analyse data in various \\nformats (including CSV, TXT, JSON, \\nParquet and Delta Lake) with the robust \\ncapabilities of Spark'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 34}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  35\\nCreating notebooks\\nWhen creating a notebook, users have two \\noptions: create a new one or import an \\nexisting one. Organisations can create a \\nnew notebook by following the familiar \\nFabric item creation workflow:\\n1. Initiate a new notebook directly from \\nthe Fabric Data Engineering  or the \\nData Science  home page, or \\nthroughout the workspace New  option. 2. Choose Import Notebook  in the \\nsame window to import an existing \\nnotebook, such as an Azure Databricks \\nnotebook file.\\n3. Once you have a notebook open, you \\ncan add code to it to write data to \\nOneLake.\\nWorking with data in OneLake is \\nstraightforward and does not involve a \\ncomplex set-up to access the data. \\nLoading data into OneLake via a Microsoft Fabric data engineering notebook\\nfrom pyspark.sql import SparkSession\\n# Initialize Spark session (assuming it\\'s not already initialized)\\nspark = SparkSession.builder.appName(\"ParkDataImport\").getOrCreate()\\n# URL to the CSV file\\ndata_url = \"https://www.dropbox.com/s/268uogek0mcypn9/park-data.\\ncsv?raw=1\"\\n# Read the CSV data directly into a Spark DataFrame\\ndf = spark.read.option(\"header\", \"true\").csv(data_url)\\n# Assuming csv_table_name, parquet_table_name, and delta_table_name are \\ndefined elsewhere in your code\\ncsv_table_name = \"park_data_csv\"\\nparquet_table_name = \"park_data_parquet\"\\ndelta_table_name = \"park_data_delta\"'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 35}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  36\\n# Save dataframe as CSV files to Files section of the default Lakehouse\\ndf.write.mode(\"overwrite\").format(\"csv\").save(\"Files/\" + csv_table_name)\\n# Save dataframe as Parquet files to Files section of the default \\nLakehouse\\ndf.write.mode(\"overwrite\").format(\"parquet\").save(\"Files/\" + parquet_\\ntable_name)\\n# Save dataframe as a delta lake, parquet table to Tables section of the \\ndefault Lakehouse\\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(delta_table_name)\\n# Save the dataframe as a delta lake, appending the data to an existing \\ntable\\n# Make sure the table exists and the schema matches to avoid errors\\ndf.write.mode(\"append\").format(\"delta\").saveAsTable(delta_table_name)\\nReading and data analysis\\nOnce the data has been successfully uploaded, try reading and analysing the data:\\n# Basic Data Analysis\\n# Count of animal sightings by type (excluding squirrels)\\nanimal_sightings = spark.sql(\"\"\"\\nSELECT Animal_Type, COUNT(*) as Total_Sightings\\nFROM park_data_view\\nWHERE Animal_Type != \\'Squirrel\\'\\nGROUP BY Animal_Type\\nORDER BY Total_Sightings DESC'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 36}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  37\\n\"\"\")\\nanimal_sightings.show()\\n# Average temperature and most common weather conditions\\navg_temp = spark.sql(\"\"\"\\nSELECT AVG(Temperature) as Average_Temperature\\nFROM park_data_view\\n\"\"\")\\navg_temp.show()\\ncommon_weather = spark.sql(\"\"\"\\nSELECT Weather, COUNT(*) as Frequency\\nFROM park_data_view\\nGROUP BY Weather\\nORDER BY Frequency DESC\\nLIMIT 5\\n\"\"\")\\ncommon_weather.show()\\n# Total count of squirrel sightings\\nsquirrel_sightings = spark.sql(\"\"\"\\nSELECT COUNT(*) as Total_Squirrel_Sightings\\nFROM park_data_view\\nWHERE Animal_Type = \\'Squirrel\\'\\n\"\"\")\\nsquirrel_sightings.show()'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 37}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  38\\nCreating and modifying a Delta table from Parquet in Azure \\nDatabricks with changes reflected in Fabric\\nAzure Databricks and Fabric provide a data lakehouse environment that allows businesses to \\naccess and analyse their data simultaneously, using different tools. This supports a wide range of \\ndata processing activities on the same set of data, making it easier for organisations to manage \\nand derive insights from their information efficiently.\\n1. Open your Azure Databricks workspace in a browser of your choice and launch a new Azure \\nDatabricks notebook.\\nFigure 7: Example notebook'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 38}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  39\\n2. Copy and paste the following script into your new notebook. Then, execute the following \\nPython script in your notebook to create a Delta table within your ADLS Gen2 account. \\nThis script reads some sample Parquet data and then writes it as a Delta table into your \\nADLS  account:\\n#python\\n# Adjust the file path to point to your sample parquet data using the \\nfollowing format: \\n\"abfss://<storage name>@<container name>.dfs.core.windows.\\nnet/<filepath>\" \\n# The line below reads Parquet files from your ADLS account \\ndf = spark.read.format(\\'Parquet\\').load(\"abfss://datasetsv1@olsdemo.dfs.\\ncore.windows.net/demo/full/dimension_city/\") \\n#This line writes the read data as Delta tables back into your ADLS \\naccount \\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"abfss://datasetsv1@\\nolsdemo.dfs.core.windows.net/demo/adb_dim_city_delta/\")\\nAnd, of course, Azure Databricks can also read the data in the ADLS account.\\n3. Azure Databricks can also modify the same sets of data that were originally created previously \\nwith Fabric. To see this in action, append some new rows to the Delta Lake tables you created \\nin OneLake:\\n# Import the necessary libraries\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import lit\\n# Initialize a Spark session\\nspark = SparkSession.builder.appName(\"AppendToDeltaTable\").\\ngetOrCreate()'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 39}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  40\\n# Define the path to your Delta Lake table in OneLake\\n# Replace \\'<your-delta-table-path>\\' with the actual path to your Delta \\nLake table\\ndelta_table_path = \"abfss://<container-name>@<storage-account-name>.\\ndfs.core.windows.net/<your-delta-table-path>\"\\n# Create a DataFrame with the new rows you want to append\\n# Replace the column names and values with those relevant to your table\\nnew_rows = [\\n    (\"NewValue1\", 10),\\n    (\"NewValue2\", 20)\\n    # Add as many rows as needed\\n]\\n# Define the schema based on your Delta Lake table structure\\n# This is an example schema; adjust it to match your table\\'s columns \\nand data types\\nschema = [\"ColumnName1\", \"ColumnName2\"]\\n# Create a DataFrame with the new data\\nnew_data_df = spark.createDataFrame(new_rows, schema)\\n# Append the new data to the Delta Lake table\\n# Ensure the table format is set to \\'delta\\' for Delta Lake \\ncompatibility\\nnew_data_df.write.format(\"delta\").mode(\"append\").save(delta_table_path)\\n# Verify by reading back the data from the Delta Lake table\\ndf = spark.read.format(\"delta\").load(delta_table_path)\\ndf.show()'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 40}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  41\\nAs the examples illustrate, a data \\nlakehouse, built on any platform with the \\nadvantages of open platforms, enables \\nenterprises to use a variety of engines to \\nwork on the same copy of the data at the \\nsame time.\\nAzure Databricks connector \\nwithin Power BI\\nThe Power BI connector for Azure \\nDatabricks provides seamless integration \\nbetween Power BI and Azure Databricks, \\nenabling organisations to connect, analyse \\nand visualise data stored in Azure \\nDatabricks with ease. This integration \\nsupports Microsoft Entra ID authentication, \\nremoving the need for administrators to \\ngenerate personal access tokens for \\nconnection. It is designed to enhance data \\nconnectivity and analysis experiences, \\nallowing for efficient and secure data \\nvisualisation directly from the data lake.\\n1. Obtain your Azure Databricks server \\nhostname and HTTP path for setting up \\nthe connection in Power BI.\\n2. Launch Power BI Desktop.\\n3. Choose Get Data  from the home screen \\nor navigate through File > Get Data .\\n4. Search for Azure Databricks .\\n5. Select Azure Databricks connector  \\nand then click Connect .6. Input the server hostname and HTTP \\npath you obtained earlier.\\n7. Decide between the Import  and \\nDirectQuery  modes for your data \\nconnectivity. For more insights into \\nthese options, consider reading about \\nthe use of DirectQuery in Power BI  \\nDesktop.\\n8. Select your preferred authentication \\nmethod:\\na. Personal Access Token : Enter \\nyour Azure Databricks personal \\naccess token.\\nb. Microsoft Entra ID : Choose Sign in  \\nand follow the prompts.\\nc. Username/Password : This option is \\ntypically not applicable.\\n9. After authentication, Power BI will \\npresent you with the Navigator  \\nwindow. Here, you can select the Azure \\nDatabricks data you wish to query. If \\nyour workspace has Unity Catalogue \\nenabled, you’ll first select a catalogue, \\nfollowed by a schema and a table.\\nFor workloads that need the processing \\npower and flexibility offered by Azure \\nDatabricks, enterprises can use the \\nadvanced visualisation capabilities of \\nPower BI along with Azure Databricks.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 41}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  42\\nAchieve excellence with Azure \\nDatabricks and Microsoft Fabric\\nThe integration of Azure Databricks and \\nMicrosoft Fabric represents a \\ntransformative approach to managing and \\nanalysing data within modern cloud \\nenvironments. Azure Databricks provides a \\nhigh-performance platform for data \\nprocessing and AI-driven analytics, while \\nFabric enhances these capabilities with \\nrobust data management tools. This \\ncombination allows organisations to \\nharness advanced analytics and AI \\nsolutions more effectively. \\nTogether, Azure Databricks and Fabric \\nstreamline analytics workloads by \\nproviding seamless data access without \\nthe need for redundant data copies. \\nThis integration supports direct queries \\nfrom Power BI, leading to improved \\nperformance and a simplified \\ndata  architecture. \\nThe synergy between Azure Databricks \\nand Fabric unlocks the full potential of the \\nlakehouse architecture, enhancing the \\nability to manage, process and analyse \\nlarge datasets more effectively. This \\nenables organisations to deploy AI \\nsolutions with ease and tackle complex \\nanalytical challenges.Furthermore, the combined capabilities of \\nAzure Databricks and Fabric address the \\nentire lifecycle of AI and machine learning \\nprojects, from data ingestion and storage \\nto analysis and deployment. This \\nintegration helps organisations maintain \\ncontrol over their data processes and \\nensure that AI initiatives are both scalable \\nand compliant with industry regulations.\\nOrganisations can enhance their data \\ngovernance, boost operational efficiency \\nand foster innovation in AI and analytics \\nwith the powerful integration of Azure \\nDatabricks and Fabric, making them well \\nequipped in the face of a data-driven \\neconomy and enabling them to achieve \\ntheir strategic goals.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 42}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  43\\nNext steps\\n•  Talk to a sales specialist .\\n•  Get more value from your data with Microsoft Fabric .\\n•  Unlock insights from your data and build AI solutions with \\nAzure Databricks .\\n•  Watch the webinar  to learn more about how Azure Databricks \\nand Microsoft Fabric are better together.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 43}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  44\\nAppendix \\nPrompt input\\nI want you to act as a data engineer. I will provide details about a \\nscenario where a customer needs to utilise data stored in ADLS Gen2. It \\nwill be accessed by Azure Databricks and Microsoft Fabric to load data \\nfor machine learning models. It will be your job to come up with an \\narchitecture to address this need.\\nAnswer from Azure OpenAI\\nTo address the scenario where a customer needs to utilise data stored \\nin Azure Data Lake Storage Gen2 (ADLS Gen2), accessed by Azure \\nDatabricks and Microsoft Fabric for loading data into machine learning \\nmodels, we’ll design an architecture that leverages these services \\nefficiently. This architecture aims to streamline data processing, \\nenhance data governance and ensure scalable machine learning \\noperations.\\nArchitecture Overview\\n •Data storage layer: Azure Data Lake Storage Gen2: Acts as the \\ncentral repository for storing raw data, processed data and \\nmachine learning artefacts. ADLS Gen2 is optimised for large-scale \\nanalytics scenarios and supports hierarchical namespace, which \\nsimplifies data management.\\n •Data processing and analytics layer: Azure Databricks: Serves \\nas the primary platform for data processing, analysis and \\nmachine learning model development. Azure Databricks provides a \\ncollaborative environment with support for multiple programming \\nlanguages and integrates seamlessly with ADLS Gen2 for direct \\naccess to data.\\n •Data management and governance layer: Microsoft Fabric: Utilised \\nfor managing and governing the data across the entire data \\nlandscape. Microsoft Fabric allows creating a unified data \\ngovernance model that encompasses data in ADLS Gen2 and data \\nprocessed or generated by Azure Databricks.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 44}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  45\\n •Machine learning operationalisation: Azure Machine Learning: \\nIntegrates with Azure Databricks for deploying and managing machine \\nlearning models. This service provides tools for model versioning, \\nmonitoring and serving, facilitating the operationalisation of \\nmachine learning models.\\nDetailed Workflow\\n1. Data ingestion: Data is ingested into ADLS Gen2 from various sources, \\nincluding structured databases, IoT devices, log files, etc. This \\ndata is stored in raw format within a hierarchical file system \\nstructure.\\n2. Data processing and exploration:\\n •Azure Databricks notebooks are used to access raw data stored \\nin ADLS Gen2. Data engineers and scientists process and explore \\nthis data using Spark, performing tasks like data cleaning, \\ntransformation and feature engineering.\\n •Processed data can be stored back into ADLS Gen2 in an optimised \\nformat (e.g., Delta Lake) for further analysis or machine learning.\\n3. Data governance and lineage:\\n •Microsoft Fabric is configured to catalogue data assets in ADLS \\nGen2 and data products generated by Azure Databricks. It provides \\na unified view of the data, enforcing policies, data quality rules \\nand security measures.\\n •Microsoft Fabric enables setting up data lineage and tracking to \\nmaintain visibility into data transformations and usage throughout \\nthe data lifecycle.\\n4. Machine learning model training and deployment:\\n •Data scientists use Azure Databricks for developing and training \\nmachine learning models, leveraging the integrated MLflow for \\nexperiment tracking and model management.\\n •Trained models are registered in Azure Machine Learning, which \\nis used to deploy these models to production as web services or \\ncontainers, enabling real-time or batch scoring.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 45}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  46\\n5. Monitoring and management:\\n •Azure Machine Learning provides model monitoring capabilities, \\ntracking model performance and data drift over time. It facilitates \\nretraining workflows when models become stale.\\n •Microsoft Fabric oversees the governance and compliance aspect, \\nensuring that data access and model deployments adhere to \\norganisational and regulatory standards.\\nThis architecture provides a comprehensive solution for leveraging \\nADLS Gen2 data in Azure Databricks and Microsoft Fabric, facilitating \\nefficient machine learning model development, governance and \\noperationalisation. It ensures scalability, security and compliance, \\ncatering to the dynamic needs of modern data-driven organisations.\\nNote\\nPrompts and answers reflected here may not reflect your exact experience.\\nPrompt input\\nBased on the above, can you provide detailed build instructions \\nwith\\xa0code?\\nAnswer from Azure OpenAI\\nCreating a detailed build guide with code for integrating Azure \\nData Lake Storage Gen2 (ADLS Gen2), Azure Databricks and Microsoft \\nFabric into a cohesive architecture for machine learning requires \\nseveral steps. Below, we’ll outline key phases in this process, \\nincluding set-up, data processing and machine learning model lifecycle \\nmanagement. This guide assumes a basic understanding of Azure services \\nand a subscription where you can deploy these services.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 46}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  47\\n1. Set-up and Configuration\\nAzure Data Lake Storage Gen2\\n1. Create an ADLS Gen2 Storage Account:\\n •Navigate to the Azure Portal.\\n •Create a new storage account, selecting your subscription, resource \\ngroup and the ‘StorageV2’ account kind.\\n •Enable the hierarchical namespace to use ADLS Gen2 features.\\n2. Configure Storage Account Access:\\n •Within the storage account, go to the ‘Access keys’ section and \\nnote down the connection string for later use.\\n •Alternatively, set up Azure Active Directory (Microsoft Entra ID) \\nauthentication for more secure access.\\nAzure Databricks\\n1. Create an Azure Databricks Workspace:\\n •In the Azure Portal, create a new Azure Databricks service.\\n •Once the deployment is complete, navigate to the Azure Databricks \\nworkspace and launch the workspace.\\n2. Create a Cluster in Azure Databricks:\\n •Inside the Azure Databricks workspace, go to ‘Compute’.\\n •Create a new cluster, ensuring it’s compatible with the libraries \\nyou plan to use.\\n3. Mount ADLS Gen2 to Azure Databricks (for direct access to data):\\n •In a Azure Databricks notebook, use the following code to mount \\nyour ADLS Gen2 storage for direct access:'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 47}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  48\\nPython \\nconfigs = { \"fs.azure.account.auth.type\": \"OAuth\", \"fs.azure.\\naccount.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.\\noauth2.ClientCredsTokenProvider\", \"fs.azure.account.oauth2.\\nclient.id\": \"<Application-ID>\", \"fs.azure.account.oauth2.client.\\nsecret\": dbutils.secrets.get(scope=\"<scope-name>\", key=\"<secret-\\nkey>\"), \"fs.azure.account.oauth2.client.endpoint\": \"https://\\nlogin.microsoftonline.com/<Directory-ID>/oauth2/token\" } dbutils.\\nfs.mount( source=\"abfss://<filesystem>@<storage-account-name>.\\ndfs.core.windows.net/\", mount_point=\"/mnt/<mount-name>\", extra_\\nconfigs=configs) \\n •Replace placeholders with your actual Microsoft Entra ID \\napplication registration details and ADLS Gen2 storage details.\\nMicrosoft Fabric\\n1. Setting Up Microsoft Fabric:\\n •Currently, Microsoft Fabric is a conceptual integration point \\nand may not have direct customer-facing set-up instructions \\navailable publicly. Typically, set-up would involve configuring \\nyour environment to catalogue and manage data assets across Azure \\nDatabricks and ADLS Gen2.\\n •For demonstration purposes, we’ll proceed to the next steps \\nassuming Microsoft Fabric is used to govern and secure data access, \\nensuring compliance and data quality.\\n2. Data Processing with Azure Databricks\\n1. Load Data from ADLS Gen2:\\n •Use the mounted path to read data into a Azure Databricks \\nDataFrame:\\npythonCopy code\\ndf = spark.read.csv(\"/mnt/<mount-name>/path/to/data.csv\") \\n2. Data Transformation:\\n •Perform necessary data transformations using Spark DataFrame \\noperations.'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 48}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  49\\n3. Save Processed Data:\\n •Save the processed data back to ADLS Gen2 in an optimised \\nformat, such as Delta Lake:\\npythonCopy code\\ndf.write.format(\"delta\").save(\"/mnt/<mount-name>/path/to/\\nprocessed/data/\")\\n3. Machine Learning Model Training and Deployment\\n1. Model Training:\\n •Use Azure Databricks to train a machine learning model using the \\nprocessed data.\\n •Example using Spark MLlib for logistic regression:\\npythonCopy code\\nfrom pyspark.ml.classification import LogisticRegression # \\nAssuming df is a DataFrame prepared for training # Train \\na model lr = LogisticRegression(featuresCol=\\'features\\', \\nlabelCol=\\'label\\') model = lr.fit(df) # Save the model model.\\nwrite().overwrite().save(\"/mnt/<mount-name>/path/to/model\") \\n2. Model Deployment:\\n •Integrate with Azure Machine Learning for model deployment, \\nmonitoring and management. This part is complex and typically \\ninvolves registering the model, creating a scoring script and \\ndeploying the model as a\\xa0web\\xa0service.\\nNote\\nPrompts and answers reflected here may not reflect your exact experience.\\n©2024 Microsoft Corporation. All rights reserved. This document is provided ‘as-is’. Information and views expressed in this document, including URLs and other \\ninternet website references, may change without notice. You bear the risk of using it. This document does not provide you with any legal rights to any intellectual \\nproperty in any Microsoft product. You may copy and use this document for your internal reference purposes.')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PDF loading for LLM to answer based on the document.\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"Azure_DataBricks and Microsoft Fabric.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: \\nAnswer the question based on the context below. If you can\\'t \\nanswer the question, reply \"I don\\'t know\".\\n\\nContext: Mary\\'s sister is Susana\\n\\nQuestion: Who is Mary\\'s sister?\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt.format(context=\"Mary's sister is Susana\", question=\"Who is Mary's sister?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Of course! Based on the context provided, Mary's sister is Susana.\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | parser\n",
    "chain.invoke({\n",
    "    \"context\": \"Mary's sister is Susana\",\n",
    "    \"question\": \"Who is Mary's sister?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'PromptInput',\n",
       " 'type': 'object',\n",
       " 'properties': {'context': {'title': 'Context', 'type': 'string'},\n",
       "  'question': {'title': 'Question', 'type': 'string'}},\n",
       " 'required': ['context', 'question']}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(pages, \n",
    "    embedding=embeddings\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 37}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  38\\nCreating and modifying a Delta table from Parquet in Azure \\nDatabricks with changes reflected in Fabric\\nAzure Databricks and Fabric provide a data lakehouse environment that allows businesses to \\naccess and analyse their data simultaneously, using different tools. This supports a wide range of \\ndata processing activities on the same set of data, making it easier for organisations to manage \\nand derive insights from their information efficiently.\\n1. Open your Azure Databricks workspace in a browser of your choice and launch a new Azure \\nDatabricks notebook.\\nFigure 7: Example notebook'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 18}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  19\\nIntegrate Azure Databricks \\nwith Power BI to enhance \\ndata  workflows\\nIntegrating Azure Databricks with Power BI \\nprovides significant advantages for data \\nmanagement and visualisation, enhancing \\nboth security and performance in data \\nanalytics workflows:\\n1. First, the integration allows more secure \\nand interactive data visualisation \\nexperiences directly from the data lake, \\navoiding the latency and costs \\nassociated with traditional data \\nprocessing workflows. It uses Microsoft \\nEntra ID for authentication, simplifying \\nthe user experience and increasing \\nsecurity, eliminating the need for \\npersonal access tokens. This integration \\nensures that security controls at the \\ndata lake level are enforced within \\nPower BI, maintaining consistent \\nsecurity policies across platforms.1\\n2. Second, the semantic lakehouse \\narchitecture streamlines data ingestion \\nand storage. It provides a unified \\nstorage layer that supports an extensive \\nrange of data formats and structures, \\nsignificantly boosting the efficiency of \\ndata processing and transformation. \\n1 Power Up your BI with Microsoft Power BI and \\nLakehouse in Azure Databricks: Part 1 – Essentials – \\nMicrosoft Community HubThis set-up not only simplifies the \\nanalytics stack, but also enhances data \\nquality and accessibility for BI tools, \\nenabling more sophisticated data \\nmodelling and analytics directly on \\nlarge  datasets.2 \\n3. Last, the integration supports advanced \\nanalytics scenarios, simplifying the \\nmanagement and analysis of large data \\nvolumes. The DirectQuery option in \\nPower BI plays a crucial role here, \\nallowing users to perform real-time \\nanalysis without moving data out of the \\nlakehouse. This capability is critical for \\nmaintaining up-to-the-minute accuracy \\nin reports and dashboards, providing \\nbusinesses with insights that are both \\ndeep and immediately actionable.3\\nThese features collectively make Azure \\nDatabricks and Power BI a robust \\ncombination, offering businesses \\nadvanced tools to harness their data \\neffectively and securely. To learn more \\nabout Power BI and Azure lakehouse \\nintegration, consult the following \\ndocument: https://learn.microsoft.com/\\nfabric/get-started/directlake-overview  \\n2 Power Up with Power BI and Lakehouse in Azure \\nDatabricks: Part 3 – Tuning Azure Databricks SQL – \\nMicrosoft Community Hub\\n3 https://docs.databricks.com/partners/bi/power-bi.html'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 25}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  26\\nData Factory and Azure \\nDatabricks activity in \\nMicrosoft Fabric\\nAzure Databricks activity in Microsoft \\nFabric represents a significant evolution in \\ndata processing within cloud environments, \\nintegrating the extensive capabilities of \\nAzure Data Factory into a more unified \\nand robust framework. With the new \\nAzure Databricks activity, users can easily \\ncreate and manage data pipelines in Fabric, \\nincorporating sophisticated analytics \\nand processing tasks directly into \\ntheir workflows. Users can configure Azure Databricks \\nclusters that are used for data processing \\ndirectly within Fabric, like the functionality \\noffered in Azure Data Factory. This \\nincludes the ability to set up Azure spot \\ninstances for accessing unused Azure \\ncompute capacity at reduced costs and \\nspecifying cluster policies to ensure that \\ncluster configurations meet organisational \\nstandards and requirements.\\nOne activity that encompasses all three job types: \\nNotebook, Jar, PythonUnity Catalog  support and \\nPolicy ID  integration\\nFigure 6: Seamless integration between Azure Databricks and Data Factory with Microsoft Fabric'),\n",
       " Document(metadata={'source': 'Azure_DataBricks and Microsoft Fabric.pdf', 'page': 17}, page_content='Synergy in Analytics: Unifying Azure Databricks and Microsoft Fabric  18\\nDirectly publish datasets \\nto Power BI workspaces:\\n● Publish from Azure UI, without \\nPower BI Desktop\\n● Publish entire schemas with table \\nrelationships (PK/FK)\\nFigure 5: Deep Power BI integration\\nThe default dataset includes all tables from the lakehouse, allowing users to establish \\nrelationships and apply various modelling changes. These datasets from Unity Catalogue can \\nbe directly published to Power BI. Users can access and edit a published semantic model with \\nthe web modelling editor that’s accessible through Power BI.\\nIn the model view within the web modelling editor, you can see whether there’s a Direct Lake \\nconnection by hovering the cursor over the table headers. Direct Lake also allows for the \\ncreation of new Power BI datasets directly through the web. This process ensures the use of \\nDirect Lake for the connection. To learn more about using the web editor for semantic models, \\nthe following document will help you get started: Edit data models in the Power BI service \\n(preview) – Power BI | Microsoft Learn .')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriver = vectorstore.as_retriever()\n",
    "retriver.invoke(\"databricks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the context provided in the document, \"Databricks\" appears to be a tool that enables users to create and manage data pipelines within a cloud environment. It integrates the capabilities of Azure Data Factory into a more unified and robust framework, allowing users to easily create and manage data pipelines directly within Microsoft Fabric. Additionally, Databricks provides a notebook-style interface for crafting Apache Spark jobs and conducting machine learning experiments, as well as offering advanced visualizations and Markdown text integration.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriver, \n",
    "        \"question\": itemgetter(\"question\") \n",
    "    }\n",
    "    | prompt \n",
    "    | model \n",
    "    | parser \n",
    ") \n",
    "\n",
    "chain.invoke({\"question\": \"what is databricks\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions: what is databricks?\n",
      "Answer:  Based on the provided context, Databricks is a platform that combines Azure Databricks and Microsoft Fabric, providing a unified solution for data management and analytics. It allows users to seamlessly switch between different platforms, offering a cohesive and powerful solution for data management and AI projects.\n",
      "\n",
      "Databricks offers centralized data storage through OneLake, which simplifies data access and governance, enabling Azure Databricks to directly utilize the data for analytical processes. The integration between Azure Databricks and Data Factory in Fabric facilitates streamlined workflows from data ingestion and validation to transformation, supporting data analytics, data science, and AI projects. Additionally, Premium Azure Databricks workspaces support credential passthrough, strengthening the security and ease of access to centralized data for further processing and analysis.\n",
      "\n",
      "Questions: How databricks makes the processing fast?\n",
      "Answer: According to the provided documents, Databricks makes the processing fast in several ways:\n",
      "\n",
      "1. Tiered storage model: By utilizing a tiered storage model within a data lakehouse environment, Databricks enables efficient data processing and management. This approach allows organizations to manage large-scale data analytics pipelines more effectively.\n",
      "2. Integration with Fabric: Integrating Azure Databricks with Fabric workloads enables the use of data stored and managed in an Azure Databricks lakehouse with the analytics tools in Fabric. This integration supports a more agile response to data-driven insights and operational needs.\n",
      "3. Utilizing OneLake data: Utilizing OneLake data in Azure Databricks with Lakehouse Federation allows users to query data across multiple storage systems within Azure Databricks, enhancing the flexibility and scalability of data analytics operations.\n",
      "4. Medallion architecture: By adopting a medallion architecture with Azure Databricks and Fabric, organizations can enable efficient data processing and management within a lakehouse environment. This approach facilitates more agile responses to data-driven insights and operational needs.\n",
      "\n",
      "In summary, Databricks makes the processing fast by utilizing a tiered storage model, integrating with Fabric, utilizing OneLake data, and adopting a medallion architecture.\n",
      "\n",
      "Questions: explain little bit more about spark\n",
      "Answer: Of course! Apache Spark is an open-source data processing engine that can handle large-scale data analysis tasks with ease. Here are some key features and capabilities of Spark:\n",
      "\n",
      "1. **Distributed architecture**: Spark is designed to run on a cluster of machines, allowing it to scale horizontally and process large amounts of data quickly.\n",
      "2. **Resilient distributed dataset (RDD)**: RDD is the core data structure in Spark, representing a set of data that can be processed in parallel across multiple nodes in a cluster. RDDs are fault-tolerant, meaning that if a node fails during processing, the task will automatically retry on another node.\n",
      "3. **Spark SQL**: Spark SQL is a query language for Hive and Spark, allowing users to perform structured queries over their data. It supports a variety of data sources, including Parquet, JSON, and CSV.\n",
      "4. **Machine learning libraries**: Spark comes with several machine learning libraries, including MLlib (Spark's built-in machine learning library), GraphXRDD, and GraphXLD. These libraries provide tools for tasks like clustering, classification, and regression.\n",
      "5. **Streaming capabilities**: Spark can handle real-time data streaming applications using its Spark Streaming API or its integration with Kafka.\n",
      "6. **Graph processing**: Spark provides a graph processing engine called GraphXRDD, which allows users to perform graph computations on large graphs.\n",
      "7. **Hive integration**: Spark is designed to work seamlessly with Hive, a popular data warehousing framework. This integration makes it easy to perform ETL (Extract, Transform, Load) operations using Spark.\n",
      "8. **SparkR**: SparkR is an open-source R package for Spark, allowing users to perform statistical analysis and machine learning tasks in R within the Spark environment.\n",
      "9. **Spark MLlib**: Spark MLlib is a library for building machine learning models in Spark. It provides a variety of algorithms for classification, regression, clustering, and more.\n",
      "10. **Spark NLP**: Spark NLP is a library for natural language processing tasks like text preprocessing, sentiment analysis, named entity recognition, and more.\n",
      "\n",
      "These are just a few examples of the many features and capabilities of Apache Spark. Its versatility and scalability make it a popular choice for data engineers and scientists working with large datasets.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\"what is databricks?\",\n",
    "             \"How databricks makes the processing fast?\",\n",
    "             \"explain little bit more about spark\"]\n",
    "for question in questions:\n",
    "    print(f\"Questions: {question}\")\n",
    "    print(f\"Answer: {chain.invoke({'question': question})}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided in the document, \"Databricks\" appears to be a product or service that offers a data lakehouse environment for businesses to access and analyze their data simultaneously using different tools. The document mentions that Azure Databricks and Microsoft Fabric provide a data lakehouse environment that allows organizations to manage and derive insights from their information efficiently, and that the integration of Azure Databricks with Power BI enhances data workflows by providing more secure and interactive data visualization experiences directly from the data lake. Additionally, the document mentions that Data Factory and Azure Databricks activity in Microsoft Fabric represents a significant evolution in data processing within cloud environments, integrating the extensive capabilities of Azure Data Factory into a more unified and robust framework.\n",
      "\n",
      "Therefore, based on the context provided, \"Databricks\" appears to be a platform or service that enables organizations to manage and analyze their data using different tools and technologies, while also providing a secure and interactive environment for data visualization and analysis."
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"question\": \"What is databricks\"}):\n",
    "    print(s, end=\"\",flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I don't know the answer to your question. The provided document does not contain enough information to answer your query. Could you please provide more context or clarify what you are asking?\",\n",
       " \"I don't know the answer to your question based on the provided document. The document provides information about the integration of Azure Databricks and Microsoft Fabric, but it doesn't contain any specific questions or answers related to your query. If you have any more context or information about the question you are asking, please let me know and I'll do my best to help.\",\n",
       " 'Based on the context provided in the document, the answer to the question \"How does Azure Databricks help businesses simplify their data architecture and reduce infrastructure complexity?\" is:\\n\\nAzure Databricks helps businesses simplify their data architecture and reduce infrastructure complexity by providing a unified storage layer that supports a wide range of data formats and structures, significantly boosting the efficiency of data processing and transformation. It also integrates with Power BI to enhance data visualization experiences directly from the data lake, avoiding latency and costs associated with traditional data processing workflows. Additionally, it simplifies the analytics stack by providing a single platform for data ingestion, storage, and analysis, making it easier for businesses to focus on creating value through AI rather than grappling with data management challenges.',\n",
       " \"I don't know the answer to your question based on the provided context. The document provides an overview of Azure Databricks and Microsoft Fabric, but it does not contain specific information about how to answer your question. Can you provide more context or clarify your question?\",\n",
       " 'Based on the provided context, the answer to the question \"How does Azure Databricks help businesses simplify data architecture and reduce infrastructure complexity?\" is:\\n\\nAzure Databricks simplifies data architecture and reduces infrastructure complexity by providing a unified storage layer that supports a wide range of data formats and structures, streamlining data ingestion and storage, and enabling advanced analytics scenarios. It also provides centralized fine-grained access control for an organization\\'s data storage locations, auditing of data access, and lineage tracking from ingestion to all data workloads. Additionally, it supports AI capabilities to automatically document tables and columns, facilitate semantic search, and make data professionals more efficient in managing data.',\n",
       " 'Based on the provided context, the answer to your question is \"I don\\'t know\". The document provides a general overview of the synergy between Azure Databricks and Microsoft Fabric, but it does not specifically address your question. To get more information on how to integrate Azure Databricks and Power BI, you can consult the following documents:\\n\\n* \"Power Up your BI with Microsoft Power BI and Lakehouse in Azure Databricks: Part 1 – Essentials\"\\n* \"Power Up with Power BI and Lakehouse in Azure Databricks: Part 3 – Tuning Azure Databricks SQL\"\\n* \"https://learn.microsoft.com/fabric/get-started/directlake-overview\"\\n\\nThese documents provide detailed instructions on how to integrate Azure Databricks and Power BI, including how to publish datasets directly from the lakehouse to Power BI workspaces.',\n",
       " 'Based on the provided context, the answer to the question \"Can Azure Databricks and Microsoft Fabric be used together?\" is YES. The documents provide information on how Azure Databricks and Microsoft Fabric can be used together to create a robust analytics stack, enhance data management and analysis, and simplify data architecture. They also highlight the benefits of using these two services together, such as improved security, increased performance, and better data governance.',\n",
       " \"I don't know the answer to your question as it is not provided in the given text. Could you please provide more context or clarify the question you are asking?\",\n",
       " \"I don't know the answer to your question based on the provided context. The document you provided does not contain a specific question or answer related to this topic. Please provide more context or clarify your question so I can better assist you.\",\n",
       " \"Based on the provided context, I can answer your question as follows:\\n\\nThe integration of Azure Databricks and Power BI provides significant advantages for data management and visualization, enhancing both security and performance in data analytics workflows. Here are some key benefits of this integration:\\n\\n1. Secure and interactive data visualization experiences directly from the data lake, avoiding latency and costs associated with traditional data processing workflows.\\n2. Simplifies the analytics stack by providing a unified storage layer that supports an extensive range of data formats and structures, significantly boosting the efficiency of data processing and transformation.\\n3. Supports advanced analytics scenarios by allowing users to perform real-time analysis without moving data out of the lakehouse, critical for maintaining up-to-the-minute accuracy in reports and dashboards.\\n4. Provides a centralized fine-grained access control system for an organization's data storage locations, auditing of data access, and lineage tracking from ingestion to all data workloads.\\n5. Allows building AI models that require massive amounts of compute power from both traditional CPUs and advanced GPUs, enabling enterprises to bring the right type of processing power to their data exploration and data modeling tasks.\\n6. Directly publish datasets to Power BI workspaces, allowing users to establish relationships and apply various modeling changes without having to move the data.\\n\\nOverall, the integration of Azure Databricks and Power BI enables enterprises to focus on creating value through AI rather than grappling with data management challenges.\",\n",
       " 'The answer to the question \"Can Azure Databricks and Microsoft Fabric be integrated to enhance data workflows?\" is yes. According to the document, integrating Azure Databricks with Power BI provides significant advantages for data management and visualization, enhancing both security and performance in data analytics workflows. The integration allows more secure and interactive data visualization experiences directly from the data lake, avoiding latency and costs associated with traditional data processing workflows. It also simplifies the analytics stack and enhances data quality and accessibility for BI tools, enabling more sophisticated data modeling and analytics directly on large datasets. Additionally, the integration supports advanced analytics scenarios, streamlining data ingestion and storage, and providing a unified storage layer that supports an extensive range of data formats and structures. Overall, the combination of Azure Databricks and Microsoft Fabric offers businesses advanced tools to harness their data effectively and securely.',\n",
       " 'Based on the provided context, I can answer your question as follows:\\n\\nThe integration of Azure Databricks and Microsoft Fabric provides significant advantages for data management and visualization, enhancing both security and performance in data analytics workflows. The integration allows more secure and interactive data visualization experiences directly from the data lake, avoiding latency and costs associated with traditional data processing workflows. It also ensures that security controls at the data lake level are enforced within Power BI, maintaining consistent security policies across platforms. Additionally, the semantic lakehouse architecture streamlines data ingestion and storage, providing a unified storage layer that supports an extensive range of data formats and structures, significantly boosting the efficiency of data processing and transformation. The integration also supports advanced analytics scenarios, simplifying the management and analysis of large data volumes.\\n\\nTo learn more about Power BI and Azure lakehouse integration, you can consult the following document: <https://learn.microsoft.com/fabric/get-started/direct-lake-connections>',\n",
       " 'Based on the provided document, the answer to the question \"Can Azure Databricks and Microsoft Fabric be used together for advanced analytics?\" is YES. The document provides several examples of how Azure Databricks and Microsoft Fabric can be used together to enhance data management and analytics workflows, including:\\n\\n1. Integrating Azure Databricks with Power BI to provide a unified storage layer that supports a wide range of data formats and structures, significantly boosting the efficiency of data processing and transformation.\\n2. Directly publishing datasets from Azure UI to Power BI workspaces without Power BI Desktop, allowing users to establish relationships and apply various modeling changes.\\n3. Using the web modelling editor in Power BI to edit data models directly within the service (preview), which ensures the use of Direct Lake for the connection.\\n4. Implementing effective data governance in a data lakehouse architecture through the implementation of policies, procedures, and standards for managing data, including defining data ownership and stewardship, establishing data quality standards, and implementing data security and compliance measures.\\n5. Building AI models that require massive amounts of compute power from both traditional CPUs and advanced GPUs, as the data lakehouse architecture empowers the use of multiple compute engines, including both Azure Databricks and Fabric, on their full data estate stored in a data lakehouse.',\n",
       " 'The answer to the question \"Can Azure Databricks and Microsoft Fabric be used together for advanced analytics?\" is yes. According to the provided document, Azure Databricks and Microsoft Fabric can be used together to create a robust combination for advanced analytics. The document highlights several benefits of using these two services together, including the ability to integrate Azure Databricks with Power BI to enhance data workflows, simplify the analytics stack, and streamline data ingestion and storage. Additionally, the integration supports advanced analytics scenarios and enables users to perform real-time analysis without moving data out of the lakehouse. Overall, the document suggests that combining Azure Databricks and Microsoft Fabric can provide businesses with advanced tools to harness their data effectively and securely.',\n",
       " \"I don't know the answer to your question based on the provided content. The document you provided doesn't contain the specific question you asked. Could you please provide more context or clarify the question you are asking?\",\n",
       " 'Based on the context provided in the document, the answer to the question \"How does Azure Databricks and Microsoft Fabric enhance data governance?\" is:\\n\\nAzure Databricks and Microsoft Fabric provide powerful features to enhance data governance in a data lakehouse architecture. These include:\\n\\n1. Unity Catalogue: Provides centralized fine-grained access control for an organization\\'s data storage locations, auditing of data access, and lineage tracking from ingestion to all data workloads.\\n2. AI capabilities: Automatically documents tables and columns, facilitates semantic search, and helps surface related data products.\\n3. Shortcuts: Allow data professionals to link data in their Unity Catalogue (with Azure Databricks shortcuts) or external ADLS accounts to OneLake, making them appear unified.\\n4. Better together: As the demand for analytics grows and data platforms evolve into more intricate systems, governing the platform – management of data availability, usability, integrity, and security – becomes paramount. In a data lakehouse architecture, data governance helps ensure that data is properly catalogued, classified, and managed.\\n\\nBy implementing effective data governance capabilities, Azure Databricks and Microsoft Fabric can help organizations manage their data properly and use it to drive business value.',\n",
       " 'Based on the provided context, the answer to your question is:\\n\\nYes, Azure Databricks and Microsoft Fabric can be integrated to enhance data management and visualization, providing significant advantages for data-intensive workloads. The integration of Azure Databricks with Power BI provides a unified storage layer that supports an extensive range of data formats and structures, significantly boosting the efficiency of data processing and transformation. This set-up not only simplifies the analytics stack but also enhances data quality and accessibility for BI tools, enabling more sophisticated data modelling and analytics directly on large datasets. Additionally, the integration supports advanced analytics scenarios, simplifying the management and analysis of large data volumes.',\n",
       " \"Based on the provided context, I can answer your question as follows:\\n\\nThe integration of Azure Databricks and Power BI provides several advantages for data management and visualization. Here are some key benefits:\\n\\n1. Secure and interactive data visualization experiences directly from the data lake, avoiding latency and costs associated with traditional data processing workflows.\\n2. Simplifies the analytics stack by providing a unified storage layer that supports an extensive range of data formats and structures, significantly boosting the efficiency of data processing and transformation.\\n3. Enhances data quality and accessibility for BI tools, enabling more sophisticated data modeling and analytics directly on large datasets.\\n4. Supports advanced analytics scenarios by allowing users to perform real-time analysis without moving data out of the lakehouse using the DirectQuery option in Power BI.\\n5. Provides centralized fine-grained access control for an organization's data storage locations, auditing of data access, and lineage tracking from ingestion to all data workloads in Azure Databricks.\\n6. Allows building AI models that require massive amounts of compute power from both traditional CPUs and advanced GPUs, using the advanced machine learning and AI capabilities of Azure Databricks and Fabric on their full data estate stored in a data lakehouse.\\n7. Directly publish datasets to Power BI workspaces, enabling users to establish relationships and apply various modeling changes without having to move the data.\\n\\nIn summary, the integration of Azure Databricks and Power BI simplifies and enhances data management and visualization workflows, providing secure, interactive, and scalable solutions for organizations leveraging AI and machine learning capabilities.\",\n",
       " \"I don't know the answer to your question based on the content provided in the documents. The documents appear to be focused on the integration of Azure Databricks and Microsoft Fabric, rather than providing information on how to answer specific questions. Could you provide more context or clarify the question you are trying to ask?\",\n",
       " 'The answer to your question is \"I don\\'t know\" because I cannot access external documents or websites to provide the information you are seeking. However, I can try to help you find the answer by providing general guidance on how to approach your query.\\n\\nBased on the context you provided, it seems that you want to know more about the integration of Azure Databricks and Microsoft Fabric, specifically in terms of data governance and AI capabilities. To learn more about these topics, I recommend consulting the following documents:\\n\\n* \"Azure Databricks and Microsoft Fabric: Synergy in Analytics\" document, which provides an overview of the integration between Azure Databricks and Microsoft Fabric, including their data governance and AI capabilities.\\n* The \"Power BI and Lakehouse Integration\" document, which provides detailed information on how to integrate Power BI with Azure Databricks and Microsoft Fabric.\\n* The \"Unity Catalogue in Azure Databricks\" document, which provides information on how Unity Catalogue can help you manage your data storage locations and audit data access.\\n\\nBy consulting these documents, you should be able to find the answers to your questions about the integration of Azure Databricks and Microsoft Fabric, as well as their data governance and AI capabilities.',\n",
       " 'Based on the provided context, the answer to the question \"How does Azure Databricks and Microsoft Fabric help organizations manage their data?\" is:\\n\\nAzure Databricks and Microsoft Fabric provide a lakehouse architecture that simplifies data management by storing Delta Lake files in an ADLS account, reducing infrastructure complexity, and enabling businesses to focus on creating value through AI rather than grappling with data management challenges. The integration of Azure Databricks with Power BI provides advanced tools to harness data effectively and securely, while the direct publishing of datasets to Power BI workspaces allows users to establish relationships and apply various modelling changes directly from the lakehouse. Additionally, the semantic lakehouse architecture streamlines data ingestion and storage, supports advanced analytics scenarios, and enhances data quality and accessibility for BI tools, making Azure Databricks and Microsoft Fabric a robust combination for managing data effectively and securely.',\n",
       " \"I don't know the answer to your question based on the provided text. The document you provided does not mention a specific question or answer related to Azure Databricks and Microsoft Fabric. Can you please provide more context or clarify your question?\",\n",
       " 'Based on the context provided in the question, I can answer it as follows:\\n\\nThe integration of Azure Databricks and Power BI provides significant advantages for data management and visualization, enhancing both security and performance in data analytics workflows. Here are some of the benefits of integrating these two platforms:\\n\\n1. Simplified data management: The integration allows more secure and interactive data visualization experiences directly from the data lake, avoiding the latency and costs associated with traditional data processing workflows.\\n2. Enhanced security: The integration uses Microsoft Entra ID for authentication, simplifying the user experience and increasing security, eliminating the need for personal access tokens. This integration ensures that security controls at the data lake level are enforced within Power BI, maintaining consistent security policies across platforms.\\n3. Improved performance: The semantic lakehouse architecture streamlines data ingestion and storage, providing a unified storage layer that supports an extensive range of data formats and structures, significantly boosting the performance of data-intensive tasks.\\n4. Deep Power BI integration: Directly publish datasets from Azure UI without Power BI Desktop, allowing users to establish relationships and apply various modeling changes. These datasets from Unity Catalogue can be directly published to Power BI, ensuring the use of Direct Lake for the connection.\\n\\nIn summary, integrating Azure Databricks and Power BI provides a more simplified, secure, and performant data management and visualization experience.',\n",
       " \"I don't know the answer to your question based on the content provided in the documents you attached. The documents appear to be about the integration of Azure Databricks and Microsoft Fabric, but they don't provide information on how to answer your question directly. Could you please provide more context or clarify your question so I can better understand and assist you?\",\n",
       " 'Based on the provided context, the answer to the question \"Can Azure Databricks and Microsoft Fabric be used together?\" is yes. The document provides several examples of how Azure Databricks and Microsoft Fabric can be used together to enhance data management, governance, and analytics capabilities. For instance, Unity Catalogue in Azure Databricks provides centralized fine-grained access control for an organization\\'s data storage locations, while Power BI can be integrated with Lakehouse in Azure Databricks to provide secure and interactive data visualization experiences directly from the data lake. Additionally, the document highlights that by simplifying data architecture and reducing infrastructure complexity, businesses can focus on creating value through AI rather than grappling with data management challenges.',\n",
       " 'Based on the provided context, the answer to the question \"What is the synergy between Azure Databricks and Microsoft Fabric?\" is:\\n\\nAzure Databricks and Microsoft Fabric provide a powerful combination of tools for data engineering, analytics, and governance. They offer a unified platform for data management, analysis, and visualization, enabling enterprises to streamline their data workflows and improve the security and performance of their data systems.\\n\\nSome of the key features of this synergy include:\\n\\n1. Integration with Power BI: Azure Databricks and Microsoft Fabric provide a seamless integration with Power BI, allowing users to perform advanced analytics and visualization directly from the data lakehouse.\\n2. DirectQuery: The DirectQuery option in Power BI enables users to query data directly from the data lakehouse, without the need for intermediate steps like ETL or caching.\\n3. Unity Catalogue: Azure Databricks\\' Unity Catalogue provides a centralized catalog of all data assets in the data lakehouse, making it easier for users to discover and access the data they need.\\n4. Governance: Microsoft Fabric offers advanced governance capabilities, including fine-grained access control, auditing, and lineage tracking, to help ensure the security and integrity of data across the organization.\\n5. Scalability: Azure Databricks and Microsoft Fabric are designed to scale with the needs of modern data workflows, providing the capacity and performance needed to handle large volumes of data and complex analytics tasks.\\n\\nOverall, the synergy between Azure Databricks and Microsoft Fabric enables enterprises to create a comprehensive data management and analysis platform that can help them make better decisions, improve operational efficiency, and drive innovation.',\n",
       " 'Based on the provided context, I can answer your question as follows:\\n\\nYes, Azure Databricks and Microsoft Fabric offer powerful modern features that provide effective data governance capabilities. These features include centralized fine-grained access control, auditing of data access, lineage tracking, and support for AI capabilities to automatically document tables and columns. Additionally, the integration between Azure Databricks and Power BI provides significant advantages for data management and visualization, including secure and interactive data visualization experiences, semantic lakehouse architecture, and advanced analytics scenarios. By simplifying data architecture and reducing infrastructure complexity, businesses can focus on creating value through AI rather than grappling with data management challenges.',\n",
       " \"I'm happy to help! Based on the context provided in the question, I can answer the following questions:\\n\\n1. How does Azure Databricks and Microsoft Fabric simplify data architecture and reduce infrastructure complexity?\\nAnswer: By simplifying data architecture and reducing infrastructure complexity, businesses can focus on creating value through AI rather than grappling with data management challenges. The lakehouse architecture stores Delta Lake files in an ADLS account, which is a cost-effective cloud storage service. Additionally, building AI models requires massive amounts of compute power from both traditional CPUs and advanced GPUs, and the data lakehouse architecture empowers the use of multiple compute engines, including both Azure Databricks and Fabric.\\n2. How does DirectQuery in Power BI play a crucial role in enhancing data analysis directly from the data lake?\\nAnswer: DirectQuery in Power BI allows users to perform real-time analysis without moving data out of the lakehouse, ensuring that security and governance policies are maintained while providing fast and interactive analysis capabilities. Users can access and edit a published semantic model directly through the web, using the web modelling editor.\\n3. What is the benefit of using AI capabilities in Azure Databricks and Fabric on full data estate stored in a data lakehouse?\\nAnswer: Enterprises can use the advanced machine learning and AI capabilities of Azure Databricks and Fabric on their full data estate stored in a data lakehouse. These tools include end-to-end experiment management and automated machine learning toolkits that can supercharge AI projects.\",\n",
       " 'Based on the provided context, the answer to the question \"Can Azure Databricks and Microsoft Fabric be used together?\" is Yes. The document provides several examples of how Azure Databricks and Microsoft Fabric can be used together to enhance data management and analytics workflows, including:\\n\\n1. Integrating Azure Databricks with Power BI to provide secure and interactive data visualization experiences directly from the data lake, eliminating latency and costs associated with traditional data processing workflows.\\n2. Using the semantic lakehouse architecture to streamline data ingestion and storage, providing a unified storage layer that supports an extensive range of data formats and structures, significantly boosting the efficiency of data processing and transformation.\\n3. Supporting advanced analytics scenarios by allowing users to perform real-time analysis without moving data out of the lakehouse using the DirectQuery option in Power BI.\\n4. Providing effective data governance capabilities by implementing policies, procedures, and standards for managing data availability, usability, integrity, and security.\\n5. Simplifying data architecture and reducing infrastructure complexity by using the advanced machine learning and AI capabilities of Azure Databricks and Fabric on their full data estate stored in a data lakehouse.',\n",
       " \"I don't know the answer to your question as it is not within my knowledge cutoff to access the specific document you provided. The document appears to be a technical guide for integrating Azure Databricks and Microsoft Fabric, but I cannot access the contents of the document beyond what you have provided in your prompt. If you have any more questions or need further assistance, please feel free to ask!\",\n",
       " \"I don't know the answer to your question as it is not within my knowledge base. The text you provided is a document from Azure DataBricks and Microsoft Fabric, which discusses the integration of Azure Databricks and Power BI. However, the document does not provide information on how to answer your specific question. If you have any other questions or need further assistance, feel free to ask!\",\n",
       " \"The answer to your question is: I don't know. I cannot answer this question as it is based on a specific context from a document, which I cannot access or provide information on. However, I can suggest ways for you to find the answer yourself by referring to the provided document or searching for relevant information online.\",\n",
       " 'Based on the provided context, the answer to the question \"How does Azure Databricks help organizations to manage their data?\" is:\\n\\nAzure Databricks provides a lakehouse architecture that simplifies data management and reduces infrastructure complexity, allowing businesses to focus on creating value through AI rather than grappling with data management challenges. It offers features such as centralized fine-grained access control, auditing of data access, lineage tracking from ingestion to all data workloads, and system tables that provide a straightforward way to query audit data, billing data, and lineage. Additionally, it integrates with Power BI to enhance data workflows and offers advanced tools for building AI models and automated machine learning toolkits.',\n",
       " 'Based on the provided context, the answer to the question \"Can Azure Databricks and Microsoft Fabric be used together?\" is yes. The document provides several examples of how Azure Databricks and Microsoft Fabric can be used together to enhance data management and analytics workflows. For instance, Azure Databricks can be integrated with Power BI to provide a unified storage layer that supports an extensive range of data formats and structures, significantly boosting the efficiency of data processing and transformation. Additionally, the DirectQuery option in Power BI allows users to perform real-time analysis without moving data out of the lakehouse, which is critical for maintaining up-to-the-minute accuracy in reports and dashboards. Overall, the document suggests that combining Azure Databricks and Microsoft Fabric can provide advanced tools for harnessing data effectively and securely.',\n",
       " 'Based on the provided context, I can answer your question as follows:\\n\\nThe document suggests that Azure Databricks and Microsoft Fabric offer powerful modern features for data governance in a data lakehouse architecture. These features include centralized fine-grained access control, auditing of data access, lineage tracking, and AI capabilities to automatically document tables and columns. Additionally, the integration of Azure Databricks with Power BI provides significant advantages for data management and visualization, enhancing security and performance in data analytics workflows.\\n\\nTherefore, the answer to your question is \"Yes,\" Azure Databricks and Microsoft Fabric offer powerful modern features for data governance in a data lakehouse architecture.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"question\": q} for q in question])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
